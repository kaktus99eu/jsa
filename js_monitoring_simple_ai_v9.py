import asyncio
import argparse  
import aiohttp
import redis.asyncio as redis
from tqdm import tqdm
import jsbeautifier
import os
import tempfile
import re
from collections import defaultdict
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning, MarkupResemblesLocatorWarning
import random
import warnings
import time
import json
import subprocess
from datetime import datetime, timezone
from email.utils import parsedate_to_datetime
import hashlib
import math
import sys
from playwright.async_api import async_playwright
import difflib
import traceback
import zlib

# [FIX] –ü–æ–¥–∞–≤–ª—è–µ–º –≤—Å–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–º–∞—é—Ç –≤—ã–≤–æ–¥ tqdm
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)
warnings.filterwarnings("ignore", category=UserWarning, module='bs4')

# ---------------- Config ----------------
SEEN_ENDPOINTS_KEY_TPL = "js_endpoints:{host}"
CONTENT_HASH_KEY_TPL = "content_hashes:{hash}"
CHECKPOINT_KEY_TPL = "checkpoint:{cycle_id}"
DEFAULT_BATCH_SIZE = 1500  # Batch size for checkpoint processing
DEFAULT_RETRY_ATTEMPTS = 2
DEFAULT_RETRY_DELAY = 5
CONCURRENT_REQUESTS = 5
LARGE_FILE_THRESHOLD = 2_000_000  # 2MB threshold for large files
LARGE_FILE_TIMEOUT = 90  # 90 seconds for large files
NORMAL_TIMEOUT = 30  # 30 seconds for normal files
AI_SERVER_URL="http://localhost:8080/analyze"
AI_SERVER_API_KEY="GPn4OnHcjdDRPVEu00HHBoRyU1PYN/3kgilKszC9fvs="

SCRIPT_BLOCKLIST_DOMAINS = {
    "google-analytics.com", "googletagmanager.com", "connect.facebook.net",
    "googleads.g.doubleclick.net", "cdn.optimizely.com", "cdn.segment.com",
    "adservice.google.com", "mc.yandex.ru", "vk.com",
    "adroll.com", "criteo.com", "twitter.com", "pinterest.com",
    "scorecardresearch.com", "adobedtm.com", "demdex.net"
}


def get_canonical_url(url: str) -> str:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç URL –≤–∏–¥–∞ '.../main.2a8f9b.js' –≤ '.../main.js'.
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ö–∞–æ—Å–∞ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.
    Inline-—Å–∫—Ä–∏–ø—Ç—ã —Å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–º #inline-script-... –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è.
    """
    try:
        parsed_url = urlparse(url)
        
        # === –ù–ê–ß–ê–õ–û –§–ò–ö–°–ê #2 ===
        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—à —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π URL –¥–ª—è inline-—Å–∫—Ä–∏–ø—Ç–∞, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –µ–≥–æ
        if parsed_url.fragment.startswith('inline-script-'):
            return url
        # === –ö–û–ù–ï–¶ –§–ò–ö–°–ê #2 ===

        path = parsed_url.path
        clean_path = path.split('?')[0].split('#')[0]
        path_without_hash = re.sub(r'[\.-]([a-f0-9]{8,}|[A-Z0-9]{8,})', '', clean_path)
        canonical_url = parsed_url._replace(path=path_without_hash, query='', fragment='').geturl()
        return canonical_url
    except Exception:
        return url # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π url

def create_beautified_diff(old_code: str, new_code: str, filename: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç diff –º–µ–∂–¥—É –¥–≤—É–º—è –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –∫–æ–¥–∞."""
    opts = jsbeautifier.default_options()
    opts.indent_size = 2
    try: beautified_old = jsbeautifier.beautify(old_code, opts)
    except: beautified_old = old_code
    try: beautified_new = jsbeautifier.beautify(new_code, opts)
    except: beautified_new = new_code
    diff_lines = difflib.unified_diff(
        beautified_old.splitlines(keepends=True),
        beautified_new.splitlines(keepends=True),
        fromfile=f'a/{filename}',
        tofile=f'b/{filename}',
    )
    return ''.join(diff_lines)

async def send_diff_to_ai(session, diff, file_path, args):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç diff –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ server.py."""
    
    # üî• –ù–û–í–û–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—Ç–∏ URL
    from urllib.parse import urlparse
    parsed = urlparse(file_path)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    page_path = parsed.path
    
    headers = {'Authorization': f'Bearer {AI_SERVER_API_KEY}', 'Content-Type': 'application/json'}
    
    # üî• –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º URL –¥–∞–Ω–Ω—ã–µ –≤ payload
    payload = {
        'diff': diff, 
        'file_path': file_path,
        'base_url': base_url,      # –ù–∞–ø—Ä–∏–º–µ—Ä: https://revenue.tinderwebstaging.com
        'page_path': page_path,    # –ù–∞–ø—Ä–∏–º–µ—Ä: /static/build/main-xxx.js
        'full_url': file_path      # –ü–æ–ª–Ω—ã–π URL –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    }
    
    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–¢–õ–ê–î–ö–ê
    print(f"\n{'='*60}")
    print(f"[DEBUG] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ AI —Å–µ—Ä–≤–µ—Ä—É:")
    print(f"  URL: {AI_SERVER_URL}")
    print(f"  API Key –∏–∑ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã: '{AI_SERVER_API_KEY}'")
    print(f"  Authorization header: '{headers['Authorization']}'")
    print(f"  Payload keys: {list(payload.keys())}")
    print(f"{'='*60}\n")
    
    try:
        async with session.post(AI_SERVER_URL, json=payload, headers=headers, timeout=300) as resp:
            print(f"[DEBUG] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: status={resp.status}")
            if resp.status == 200:
                result = await resp.json()
                print(f"[DEBUG] –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç AI")
                return result
            else:
                error_text = await resp.text()
                print(f"[!] AI_ERROR: Status: {resp.status}, Body: {error_text[:200]}", file=sys.stderr)
                return None
    except Exception as e:
        print(f"[!] AI_ERROR: Exception: {type(e).__name__}: {e}", file=sys.stderr)
        return None

REALISTIC_USER_AGENTS = [
    # Chrome Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    # Firefox Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    # Chrome macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Safari macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    # Firefox macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    # Chrome Linux
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

# --- ENHANCED REGEX PATTERNS FOR MODERN WEB APPS ---

# ============================================================================
# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –•–†–ê–ù–ò–õ–ò–©–ï –î–õ–Ø DIFF-–ê–ù–ê–õ–ò–ó–ê 
# ============================================================================


class HybridDiffStorage:
    """
    –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ:
    - Redis: —Ö–µ—à–∏ + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–ª–µ–≥–∫–∏–µ)
    - Disk: —Å–∂–∞—Ç—ã–µ —Ñ–∞–π–ª—ã (—Ç—è–∂–µ–ª—ã–µ)
    """
    
    def __init__(self, redis_client: redis.Redis, cache_dir: str = "/tmp/js_cache"):
        self.r = redis_client
        self.cache_dir = cache_dir
        self.compression_level = 6
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∫–µ—à–∞
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # TTL —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (–≤ –¥–Ω—è—Ö)
        self.TTL_FREQUENT = 14
        self.TTL_MODERATE = 7
        self.TTL_RARE = 3
    
    def _get_file_path(self, url_hash: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–∞ –¥–∏—Å–∫–µ."""
        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–ø–∞–ø–∫–∏ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (a/b/abcd1234...)
        subdir = os.path.join(self.cache_dir, url_hash[:2], url_hash[2:4])
        os.makedirs(subdir, exist_ok=True)
        return os.path.join(subdir, f"{url_hash}.zlib")
    
    async def save_js_content(self, url: str, content: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç: —Ö–µ—à –≤ Redis, —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫."""
        canonical_url = get_canonical_url(url)
        url_hash = hashlib.md5(canonical_url.encode('utf-8', 'ignore')).hexdigest()
        
        # –ö–ª—é—á–∏ –≤ Redis (—Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ!)
        hash_key = f"js:hash:{url_hash}"
        access_key = f"js:access:{url_hash}"
        
        # 1. –°–∂–∏–º–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        try:
            compressed = zlib.compress(content.encode('utf-8', 'ignore'), self.compression_level)
        except Exception as e:
            print(f"[!] Compression failed for {url}: {e}", file=sys.stderr)
            compressed = content.encode('utf-8', 'ignore')
        
        # 2. –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à
        content_hash = hashlib.sha256(content.encode('utf-8', 'ignore')).hexdigest()
        
        # 3. üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –Ω–∞ –î–ò–°–ö
        file_path = self._get_file_path(url_hash)
        try:
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(compressed)
        except Exception as e:
            print(f"[!] Disk write failed for {url}: {e}", file=sys.stderr)
            return content_hash
        
        # 4. üóÑÔ∏è –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –•–ï–®–ò –≤ Redis
        try:
            access_count = int(await self.r.get(access_key) or 0)
        except:
            access_count = 0
        
        # –£–º–Ω—ã–π TTL
        if access_count > 5:
            ttl_seconds = 86400 * self.TTL_FREQUENT
        elif access_count > 2:
            ttl_seconds = 86400 * self.TTL_MODERATE
        else:
            ttl_seconds = 86400 * self.TTL_RARE
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Redis –¢–û–õ–¨–ö–û —Ö–µ—à –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        try:
            pipe = self.r.pipeline()
            pipe.set(hash_key, content_hash, ex=ttl_seconds)
            pipe.incr(access_key)
            pipe.expire(access_key, 86400 * 30)
            await pipe.execute()
        except Exception as e:
            print(f"[!] Redis save failed for {url}: {e}", file=sys.stderr)
        
        return content_hash
    
    async def get_and_compare(self, url: str, new_content: str) -> tuple:
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è, —Å–æ–∑–¥–∞–µ—Ç diff."""
        canonical_url = get_canonical_url(url)
        url_hash = hashlib.md5(canonical_url.encode('utf-8', 'ignore')).hexdigest()
        
        hash_key = f"js:hash:{url_hash}"
        
        # 1. üîç –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É (–ë–ï–ó –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞!)
        new_hash = hashlib.sha256(new_content.encode('utf-8', 'ignore')).hexdigest()
        
        try:
            old_hash_bytes = await self.r.get(hash_key)
            old_hash = old_hash_bytes.decode() if old_hash_bytes else None
        except Exception:
            old_hash = None
        
        if old_hash and old_hash == new_hash:
            # –§–∞–π–ª –ù–ï –∏–∑–º–µ–Ω–∏–ª—Å—è - –≤—ã—Ö–æ–¥–∏–º –ë–ï–ó –∑–∞–≥—Ä—É–∑–∫–∏ —Å –¥–∏—Å–∫–∞!
            try:
                await self.r.expire(hash_key, 86400 * self.TTL_MODERATE)
            except:
                pass
            return False, "", old_hash
        
        # 2. üìÇ –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è - –∑–∞–≥—Ä—É–∂–∞–µ–º –°–¢–ê–†–£–Æ –≤–µ—Ä—Å–∏—é —Å –¥–∏—Å–∫–∞
        file_path = self._get_file_path(url_hash)
        
        if not os.path.exists(file_path):
            # –ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è —Ñ–∞–π–ª–∞
            await self.save_js_content(url, new_content)
            return True, "", ""
        
        try:
            async with aiofiles.open(file_path, 'rb') as f:
                compressed_old = await f.read()
            old_content = zlib.decompress(compressed_old).decode('utf-8', 'ignore')
        except Exception as e:
            print(f"[!] Disk read failed for {url}: {e}", file=sys.stderr)
            await self.save_js_content(url, new_content)
            return True, "", old_hash if old_hash else ""
        
        # 3. ‚ú® –°–æ–∑–¥–∞–µ–º diff
        opts = jsbeautifier.default_options()
        opts.indent_size = 2
        
        try:
            beautified_old = jsbeautifier.beautify(old_content, opts)
        except:
            beautified_old = old_content
        
        try:
            beautified_new = jsbeautifier.beautify(new_content, opts)
        except:
            beautified_new = new_content
        
        diff_lines = difflib.unified_diff(
            beautified_old.splitlines(keepends=True),
            beautified_new.splitlines(keepends=True),
            fromfile=f'a/{url}',
            tofile=f'b/{url}',
        )
        diff = ''.join(diff_lines)
        
        # 4. üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ù–û–í–£–Æ –≤–µ—Ä—Å–∏—é
        await self.save_js_content(url, new_content)
        
        return True, diff, old_hash if old_hash else ""
    
    async def get_storage_stats(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: Redis + Disk."""
        try:
            redis_info = await self.r.info('memory')
            total_keys = await self.r.dbsize()
            hash_keys = len(await self.r.keys('js:hash:*'))
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏—Å–∫–∞
            disk_files = 0
            disk_size = 0
            for root, dirs, files in os.walk(self.cache_dir):
                disk_files += len(files)
                for file in files:
                    try:
                        disk_size += os.path.getsize(os.path.join(root, file))
                    except:
                        pass
            
            return {
                'redis_memory': redis_info.get('used_memory_human', 'N/A'),
                'redis_peak': redis_info.get('used_memory_peak_human', 'N/A'),
                'redis_keys': total_keys,
                'redis_hash_keys': hash_keys,
                'disk_files': disk_files,
                'disk_size_mb': round(disk_size / 1024 / 1024, 2),
                'fragmentation': redis_info.get('mem_fragmentation_ratio', 'N/A')
            }
        except Exception as e:
            return {'error': str(e)}



# === –ß–ï–õ–û–í–ï–ö–û–ü–û–î–û–ë–ù–´–ï –ó–ê–î–ï–†–ñ–ö–ò ===
class HumanLikeTiming:
    def __init__(self):
        self.last_request_time = 0
        self.session_start = time.time()
        self.request_count = 0
    
    async def get_delay(self) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É"""
        self.request_count += 1
        current_time = time.time()
        base_delay = random.uniform(0.5, 1.0)
        session_duration = current_time - self.session_start
        fatigue_factor = min(1.5, 1 + (session_duration / 3600))
        if random.random() < 0.10:
            base_delay += random.uniform(5.0, 15.0)
        if random.random() < 0.01:
            base_delay += random.uniform(30.0, 120.0)
        total_delay = base_delay * fatigue_factor
        elapsed = current_time - self.last_request_time
        if elapsed < total_delay:
            additional_wait = total_delay - elapsed
            await asyncio.sleep(additional_wait)
        self.last_request_time = time.time()
        return total_delay

# === –†–ê–°–®–ò–†–ï–ù–ù–´–ï HEADERS ===
class SmartHeaders:
    def __init__(self):
        self.session_cookies = {}
        self.last_referer = None
        
    def get_headers(self, url: str, is_ajax: bool = False) -> dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ headers"""
        ua = random.choice(REALISTIC_USER_AGENTS)
        headers = {
            'User-Agent': ua,
            'Accept-Language': random.choice(['en-US,en;q=0.9', 'en-US,en;q=0.9,ru;q=0.8', 'en-GB,en;q=0.9,en-US;q=0.8']),
            'Accept-Encoding': 'gzip, deflate, br', 'DNT': '1', 'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1',
        }
        if is_ajax:
            headers['Accept'] = 'application/json, text/javascript, */*; q=0.01'
            headers['X-Requested-With'] = 'XMLHttpRequest'
        else:
            headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        if self.last_referer:
            headers['Referer'] = self.last_referer
        headers['Sec-Fetch-Dest'] = 'document' if not is_ajax else 'empty'
        headers['Sec-Fetch-Mode'] = 'navigate' if not is_ajax else 'cors'
        headers['Sec-Fetch-Site'] = 'none' if not self.last_referer else 'same-origin'
        if 'Chrome' in ua:
            headers['sec-ch-ua'] = '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"'
            headers['sec-ch-ua-mobile'] = '?0'
            headers['sec-ch-ua-platform'] = '"Windows"'
        return headers
    
    def update_session(self, response_headers: dict, url: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏"""
        self.last_referer = url
        if 'Set-Cookie' in response_headers:
            cookie_value = response_headers['Set-Cookie'].split(';')[0]
            cookie_name, cookie_val = cookie_value.split('=', 1)
            self.session_cookies[cookie_name] = cookie_val
            
class WAFDetector:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø—ã WAF –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
    
    WAF_SIGNATURES = {
        'cloudflare_challenge': [
            r'checking your browser',
            r'cloudflare',
            r'cf-ray',
            r'__cf_bm',
            r'challenge-platform'
        ],
        'akamai_block': [
            r'reference #\d+\.\w+\.\d+',
            r'akamai',
            r'access denied'
        ],
        'generic_js_challenge': [
            r'please enable javascript',
            r'javascript is required',
            r'browser check',
            r'security check'
        ]
    }
    
    @staticmethod
    def detect_waf_type(response_text: str, headers: dict, status_code: int) -> tuple:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (is_blocked, waf_type, needs_browser)
        """
        response_lower = response_text.lower()
        headers_lower = {k.lower(): v.lower() for k, v in headers.items()}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if 'server' in headers_lower:
            server = headers_lower['server']
            if 'cloudflare' in server:
                # –ò—â–µ–º JS challenge
                if any(re.search(pattern, response_lower) for pattern in WAFDetector.WAF_SIGNATURES['cloudflare_challenge']):
                    return True, 'cloudflare_challenge', True
            elif 'akamaighost' in server:
                return True, 'akamai_block', True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–¥—ã
        if status_code in [403, 406, 429]:
            # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ JS challenge
            if any(re.search(pattern, response_lower) for pattern in WAFDetector.WAF_SIGNATURES['generic_js_challenge']):
                return True, 'generic_js_challenge', True
            return True, 'http_block', False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ JS challenges
        for waf_type, patterns in WAFDetector.WAF_SIGNATURES.items():
            if any(re.search(pattern, response_lower) for pattern in patterns):
                return True, waf_type, True
        
        return False, None, False
        
# –î–û–ë–ê–í–ò–¢–¨ –≠–¢–ò –ö–õ–ê–°–°–´ –í –í–ê–® –ö–û–î (–ø–æ—Å–ª–µ WAFDetector, –ø–µ—Ä–µ–¥ BrowserHandler)

class OptimizedBrowserHandler:
    """–ï–¥–∏–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä-—Ö—ç–Ω–¥–ª–µ—Ä —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –Ω–µ–Ω—É–∂–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
    
    def __init__(self, browser_semaphore: asyncio.Semaphore):
        self.browser = None
        self.context = None
        self.semaphore = browser_semaphore
        self.playwright = None
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'blocked_resources_count': 0
        }
    
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-gpu',
                '--disable-accelerated-2d-canvas',
                '--disable-software-rasterizer',
                '--memory-pressure-off',
                '--max_old_space_size=1024',  # 1GB –ª–∏–º–∏—Ç –¥–ª—è V8
                '--disable-background-timer-throttling',
                '--disable-renderer-backgrounding',
                '--disable-backgrounding-occluded-windows',
                '--disable-ipc-flooding-protection'
            ]
        )
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        self.context = await self.browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent=random.choice(REALISTIC_USER_AGENTS),
            ignore_https_errors=True
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def fetch_with_browser(self, url: str, timeout: int = 45) -> tuple:
        """
        –ö–õ–Æ–ß–ï–í–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –±–ª–æ–∫–∏—Ä—É–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        """
        async with self.semaphore:
            page = None
            self.stats['total_requests'] += 1
            
            try:
                page = await self.context.new_page()
                
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –±–ª–æ–∫–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
                async def block_resources(route):
                    resource_type = route.request.resource_type
                    blocked_types = {
                        "image", "media", "font", "other", 
                        "stylesheet"  # –ë–ª–æ–∫–∏—Ä—É–µ–º CSS –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏, JS –Ω–∞–º –≤–∞–∂–Ω–µ–µ
                    }
                    
                    if resource_type in blocked_types:
                        self.stats['blocked_resources_count'] += 1
                        await route.abort()
                    else:
                        await route.continue_()
                
                await page.route("**/*", block_resources)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                response = await page.goto(
                    url, 
                    wait_until='domcontentloaded',  # –ù–µ –∂–¥–µ–º –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                    timeout=timeout * 1000
                )
                
                if not response:
                    return None, url, False
                
                # –£–º–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ, –ø–æ—Ç–æ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ challenge
                await page.wait_for_timeout(random.randint(1500, 3000))
                content = await page.content()
                final_url = page.url
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ WAF challenge
                is_blocked, waf_type, needs_wait = WAFDetector.detect_waf_type(
                    content, dict(response.headers), response.status
                )
                
                if is_blocked and needs_wait:
                    print(f"[BROWSER] {waf_type} detected for {url}, waiting longer...")
                    await page.wait_for_timeout(random.randint(3000, 6000))
                    content = await page.content()
                    final_url = page.url
                
                self.stats['successful_requests'] += 1
                return content, final_url, True
                
            except Exception as e:
                print(f"[BROWSER] Error fetching {url}: {type(e).__name__}: {e}")
                self.stats['failed_requests'] += 1
                return None, url, False
            finally:
                if page and not page.is_closed():
                    await page.close()
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –∫–∞–∂–¥—ã–µ 20 –∑–∞–ø—Ä–æ—Å–æ–≤
                if self.stats['total_requests'] % 20 == 0:
                    await self._force_gc()
    
    async def _force_gc(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –≤ –±—Ä–∞—É–∑–µ—Ä–µ"""
        try:
            # –ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∏ –≤—ã–∑—ã–≤–∞–µ–º —Å–±–æ—Ä–∫—É –º—É—Å–æ—Ä–∞
            for context in self.browser.contexts:
                for page in context.pages:
                    if not page.is_closed():
                        await page.evaluate('window.gc && window.gc()')
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ GC
    
    def print_stats(self):
        if self.stats['total_requests'] > 0:
            success_rate = (self.stats['successful_requests'] / self.stats['total_requests']) * 100
            print(f"[BROWSER STATS] Total requests: {self.stats['total_requests']}")
            print(f"[BROWSER STATS] Success rate: {success_rate:.1f}%")
            print(f"[BROWSER STATS] Blocked resources: {self.stats['blocked_resources_count']}")


class SmartCachingHybridFetcher:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π HybridFetcher —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ retry –ª–æ–≥–∏–∫–æ–π"""
    
    def __init__(self, session: aiohttp.ClientSession, browser_handler: OptimizedBrowserHandler, r: redis.Redis):
        self.session = session
        self.browser_handler = browser_handler
        self.redis = r
        self.stats = {
            'aiohttp_success': 0,
            'browser_fallback': 0,
            'total_blocked': 0,
            'total_requests': 0,
            'cache_hits': 0,
            'retry_successes': 0
        }
    
    def get_varied_headers(self, url: str, attempt: int = 0) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ–ø—ã—Ç–∫–∏"""
        ua = random.choice(REALISTIC_USER_AGENTS)
        
        headers = {
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice([
                'en-US,en;q=0.9',
                'en-US,en;q=0.9,ru;q=0.8',
                'en-GB,en;q=0.9,en-US;q=0.8'
            ]),
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # –í–∞—Ä–∏–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–ø—ã—Ç–∫–∏
        if attempt > 0:
            headers['Cache-Control'] = random.choice(['no-cache', 'no-store', 'max-age=0'])
        
        if attempt > 1:
            headers['Pragma'] = 'no-cache'
            headers['X-Forwarded-For'] = f"192.168.{random.randint(1,254)}.{random.randint(1,254)}"
        
        # Chrome-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if 'Chrome' in ua:
            headers['sec-ch-ua'] = '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"'
            headers['sec-ch-ua-mobile'] = '?0'
            headers['sec-ch-ua-platform'] = '"Windows"'
            headers['Sec-Fetch-Dest'] = 'document'
            headers['Sec-Fetch-Mode'] = 'navigate'
            headers['Sec-Fetch-Site'] = 'none'
        
        return headers
    
    async def fetch_hybrid(self, url: str, headers: dict, timeout: int = 30) -> tuple:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ retry –ª–æ–≥–∏–∫–æ–π
        """
        self.stats['total_requests'] += 1
        initial_content = None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_key = f"browser_cache:{url_hash}"
        
        try:
            cached_content = await self.redis.get(cache_key)
            if cached_content:
                self.stats['cache_hits'] += 1
                return cached_content.decode('utf-8', 'ignore'), url, False, None
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∫—ç—à–∞
        
        # –®–∞–≥ 1: –£–º–Ω—ã–µ retry –¥–ª—è aiohttp
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # –í–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                varied_headers = self.get_varied_headers(url, attempt)
                varied_headers.update(headers)  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ timeout
                attempt_timeout = timeout + (attempt * 5)
                
                async with self.session.get(
                    url, 
                    headers=varied_headers, 
                    timeout=attempt_timeout, 
                    ssl=False, 
                    allow_redirects=True
                ) as resp:
                    
                    content = await resp.text(encoding='utf-8', errors='ignore')
                    initial_content = content
                    response_headers = dict(resp.headers)
                    status_code = resp.status
                    final_url = str(resp.url)
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ rate limiting
                    if status_code == 429:
                        retry_after = response_headers.get('Retry-After', '5')
                        try:
                            wait_time = min(int(retry_after), 30)  # –ú–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫
                        except ValueError:
                            wait_time = 5
                        
                        print(f"[RETRY] Rate limited. Waiting {wait_time}s (attempt {attempt + 1})")
                        await asyncio.sleep(wait_time + random.uniform(0, 2))
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ WAF –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                    is_blocked, waf_type, needs_browser = WAFDetector.detect_waf_type(
                        content, response_headers, status_code
                    )
                    
                    if not is_blocked:
                        # –£—Å–ø–µ—Ö! –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è
                        try:
                            await self.redis.set(f"aiohttp_cache:{url_hash}", content.encode('utf-8'), ex=300)  # 5 –º–∏–Ω
                        except Exception:
                            pass
                        
                        self.stats['aiohttp_success'] += 1
                        if attempt > 0:
                            self.stats['retry_successes'] += 1
                        return content, final_url, False, None
                    
                    # –ï—Å–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞, –Ω–æ –Ω–µ –Ω—É–∂–µ–Ω –±—Ä–∞—É–∑–µ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ—Å—Ç–æ–π 403)
                    if not needs_browser:
                        if attempt < max_retries - 1:
                            # –ü—Ä–æ–±—É–µ–º —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                            delay = (2 ** attempt) + random.uniform(1, 3)
                            print(f"[RETRY] HTTP block detected. Retrying in {delay:.1f}s (attempt {attempt + 1})")
                            await asyncio.sleep(delay)
                            continue
                        else:
                            # –ò—Å—á–µ—Ä–ø–∞–ª–∏ –ø–æ–ø—ã—Ç–∫–∏
                            self.stats['total_blocked'] += 1
                            return None, final_url, False, initial_content
                    
                    # –ù—É–∂–µ–Ω –±—Ä–∞—É–∑–µ—Ä –¥–ª—è JS challenge
                    break
                    
            except asyncio.TimeoutError:
                if attempt < max_retries - 1:
                    delay = random.uniform(2, 5) * (attempt + 1)
                    print(f"[RETRY] Timeout. Retrying in {delay:.1f}s (attempt {attempt + 1})")
                    await asyncio.sleep(delay)
                    continue
                else:
                    print(f"[RETRY] All aiohttp attempts failed for {url}")
                    break
            except Exception as e:
                if attempt < max_retries - 1:
                    delay = random.uniform(1, 3) * (attempt + 1)
                    await asyncio.sleep(delay)
                    continue
                else:
                    print(f"[RETRY] aiohttp completely failed for {url}: {e}")
                    break
        
        # –®–∞–≥ 2: Fallback –Ω–∞ –±—Ä–∞—É–∑–µ—Ä
        print(f"[HYBRID] Browser fallback for {url}")
        self.stats['browser_fallback'] += 1
        
        try:
            content, final_url, success = await self.browser_handler.fetch_with_browser(url, timeout + 15)
            
            if success and content:
                # –ö—ç—à–∏—Ä—É–µ–º –±—Ä–∞—É–∑–µ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¥–æ–ª—å—à–µ (–æ–Ω–∏ –¥–æ—Ä–æ–∂–µ –ø–æ–ª—É—á–∞—é—Ç—Å—è)
                try:
                    await self.redis.set(cache_key, content.encode('utf-8'), ex=1800)  # 30 –º–∏–Ω
                except Exception:
                    pass
                
                return content, final_url, True, initial_content
        except Exception as e:
            print(f"[HYBRID] Browser fallback failed for {url}: {e}")
        
        return None, url, False, initial_content
    
    def print_stats(self):
        total = self.stats['total_requests']
        if total > 0:
            aiohttp_pct = (self.stats['aiohttp_success'] / total) * 100
            browser_pct = (self.stats['browser_fallback'] / total) * 100
            cache_hit_pct = (self.stats['cache_hits'] / total) * 100
            
            print(f"\n[HYBRID STATS] Total requests: {total}")
            print(f"[HYBRID STATS] aiohttp success: {self.stats['aiohttp_success']} ({aiohttp_pct:.1f}%)")
            print(f"[HYBRID STATS] Browser usage: {self.stats['browser_fallback']} ({browser_pct:.1f}%)")
            print(f"[HYBRID STATS] Cache hits: {self.stats['cache_hits']} ({cache_hit_pct:.1f}%)")
            print(f"[HYBRID STATS] Retry successes: {self.stats['retry_successes']}")


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
async def init_balanced_hybrid_system(session: aiohttp.ClientSession, r: redis.Redis, 
                                    max_browser_concurrency: int = 1) -> tuple:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    browser_semaphore = asyncio.Semaphore(max_browser_concurrency)
    browser_handler = OptimizedBrowserHandler(browser_semaphore)
    await browser_handler.__aenter__()
    
    hybrid_fetcher = SmartCachingHybridFetcher(session, browser_handler, r)
    
    return browser_handler, hybrid_fetcher

async def cleanup_balanced_system(browser_handler: OptimizedBrowserHandler, 
                                hybrid_fetcher: SmartCachingHybridFetcher):
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
    hybrid_fetcher.print_stats()
    browser_handler.print_stats()
    await browser_handler.__aexit__(None, None, None)

class BrowserHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è headless –±—Ä–∞—É–∑–µ—Ä–∞"""
    
    def __init__(self):
        self.browser = None
        self.context = None
    
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def fetch_with_browser(self, url: str, timeout: int = 30) -> tuple:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (content, final_url, success)
        """
        try:
            page = await self.context.new_page()
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS
            response = await page.goto(url, wait_until='domcontentloaded', timeout=timeout * 1000)
            
            if not response:
                return None, url, False
            
            # –ñ–¥–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JS challenges
            await page.wait_for_timeout(random.randint(2000, 5000))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–∏–ª–æ –ª–∏ –Ω–∞—Å –Ω–∞ challenge —Å—Ç—Ä–∞–Ω–∏—Ü—É
            current_url = page.url
            content = await page.content()
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ challenge, –∂–¥–µ–º –¥–æ–ª—å—à–µ
            is_blocked, waf_type, _ = WAFDetector.detect_waf_type(content, dict(response.headers), response.status)
            if is_blocked:
                print(f"[BROWSER] Still blocked after initial wait, trying longer wait for {url}")
                await page.wait_for_timeout(random.randint(5000, 10000))
                content = await page.content()
                current_url = page.url
            
            await page.close()
            return content, current_url, True
            
        except Exception as e:
            if 'page' in locals():
                await page.close()
            print(f"[BROWSER] Error fetching {url}: {e}")
            return None, url, False

class HybridFetcher:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ñ–µ—Ç—á–µ—Ä: aiohttp + –±—Ä–∞—É–∑–µ—Ä –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    
    def __init__(self, session: aiohttp.ClientSession):
        self.session = session
        self.browser_handler = None
        self.stats = {
            'aiohttp_success': 0,
            'browser_fallback': 0,
            'total_blocked': 0,
            'total_requests': 0
        }
    
    async def fetch_hybrid(self, url: str, headers: dict, timeout: int = 30) -> tuple:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ñ–µ—Ç—á–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (content, final_url, used_browser)
        """
        self.stats['total_requests'] += 1
        initial_content = None 
        
        # –®–∞–≥ 1: –ü—Ä–æ–±—É–µ–º –±—ã—Å—Ç—Ä—ã–π aiohttp
        try:
            async with self.session.get(url, headers=headers, timeout=timeout, ssl=False, allow_redirects=True) as resp:
                content = await resp.text(encoding='utf-8', errors='ignore')
                initial_content = content
                response_headers = dict(resp.headers)
                status_code = resp.status
                final_url = str(resp.url)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç WAF –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                is_blocked, waf_type, needs_browser = WAFDetector.detect_waf_type(
                    content, response_headers, status_code
                )
                
                if not is_blocked:
                    # –£—Å–ø–µ—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥
                    self.stats['aiohttp_success'] += 1
                    return content, final_url, False, None
                
                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞
                self.stats['total_blocked'] += 1
                print(f"[HYBRID] WAF detected: {waf_type} for {url}, needs_browser: {needs_browser}")
                
                if not needs_browser:
                    # HTTP –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –±–µ–∑ JS challenge - –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–º–æ–∂–µ—Ç
                    return None, final_url, False, initial_content
                
        except Exception as e:
            print(f"[HYBRID] aiohttp failed for {url}: {e}")
        
        # –®–∞–≥ 2: Fallback –Ω–∞ –±—Ä–∞—É–∑–µ—Ä –¥–ª—è JS challenges
        print(f"[HYBRID] Falling back to browser for {url}")
        self.stats['browser_fallback'] += 1
        
        if not self.browser_handler:
            self.browser_handler = BrowserHandler()
            await self.browser_handler.__aenter__()
        
        try:
            content, final_url, success = await self.browser_handler.fetch_with_browser(url, timeout)
            if success:
                return content, final_url, True, initial_content 
        except Exception as e:
            print(f"[HYBRID] Browser fallback failed for {url}: {e}")
        
        return None, url, False, initial_content 
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        if self.browser_handler:
            await self.browser_handler.__aexit__(None, None, None)
    
    def print_stats(self):
        """–ü–µ—á–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        total = self.stats['total_requests']
        if total > 0:
            aiohttp_pct = (self.stats['aiohttp_success'] / total) * 100
            browser_pct = (self.stats['browser_fallback'] / total) * 100
            blocked_pct = (self.stats['total_blocked'] / total) * 100
            
            print(f"\n[HYBRID STATS] Total requests: {total}")
            print(f"[HYBRID STATS] aiohttp success: {self.stats['aiohttp_success']} ({aiohttp_pct:.1f}%)")
            print(f"[HYBRID STATS] Browser fallback: {self.stats['browser_fallback']} ({browser_pct:.1f}%)")
            print(f"[HYBRID STATS] Total blocked: {self.stats['total_blocked']} ({blocked_pct:.1f}%)")

# –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –≤–∞—à–µ–≥–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–¥–∞
async def balanced_stealth_fetch_content(session: aiohttp.ClientSession, url: str, semaphore: asyncio.Semaphore, 
                                       args, timing, header_manager, hybrid_fetcher: SmartCachingHybridFetcher, 
                                       referer: str = None):
    """
    –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ –¥–ª—è hybrid_stealth_fetch_content —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    """
    async with semaphore:
        await timing.get_delay()
        headers = header_manager.get_headers(url)
        if referer:
            headers['Referer'] = referer
        
        try:
            content, final_url, used_browser, initial_content = await hybrid_fetcher.fetch_hybrid(url, headers)
            
            if content:
                if not used_browser:
                    header_manager.update_session({}, final_url)
                
                if args.debug:
                    method = "BROWSER" if used_browser else "AIOHTTP"
                    cache_status = "CACHED" if hybrid_fetcher.stats['cache_hits'] > 0 else ""
                    print(f"[DEBUG] {method} {cache_status} fetch success for {url}")
                
                return content, final_url, initial_content
            
        except Exception as e:
            if args.debug:
                print(f"[DEBUG][Balanced Fetch] Error for {url}: {e}")
    
    return None, url, None


# Original LinkFinder pattern (legacy support)
LINKFINDER_REGEX_STR = r"""
    (?:"|'|`)                                   # Match a starting quote: ", ' or `
    (                                           # Start capturing group 1
      /                                         # The path MUST start with a slash
      [a-zA-Z0-9_?&=\/\-\#\.]+                  # Match allowed characters in the path.
    )                                           # End capturing group 1
    (?:"|'|`)                                   # Match a closing quote: ", ' or `
"""

# Modern JavaScript patterns
MODERN_JS_PATTERNS = [
    # Template literals with variables: `${baseUrl}/api/users`
    r'`([^`]*\$\{[^}]+\}[^`]*)`',
    
    # Dynamic imports: import('./routes/' + pageName + '.js')
    r'import\s*\(\s*["\']([^"\']+)["\']',
    
    # Modern API patterns: '/api/v' + version + '/users'
    r'["\']([^"\']*\/api\/[^"\']*)["\']',
    
    # REST API endpoints: '/v2/accounts/me'
    r'["\'](\/(v\d+|api|rest|graphql)\/[^"\']+)["\']',
    
    # GraphQL specific
    r'["\']([^"\']*graphql[^"\']*)["\']',
    
    # Next.js API routes: '/api/users/[id]'
    r'["\']([^"\']*\/api\/[^"\']*\[[^\]]+\][^"\']*)["\']',
    
    # Webpack/Vite chunks: '/_next/static/chunks/[id].js'
    r'["\']([^"\']*\/_next\/[^"\']+)["\']',
    r'["\']([^"\']*\/chunks\/[^"\']+)["\']',
    r'["\']([^"\']*\/assets\/[^"\']+)["\']',
    
    # Service worker paths
    r'["\']([^"\']*\/sw\.js[^"\']*)["\']',
    r'["\']([^"\']*service-worker[^"\']*)["\']',
    
    # Dynamic path construction: baseUrl + '/users/' + id
    r'[+\s]["\']([^"\']*\/[^"\']+)["\']',
    
    # Fetch/axios calls: fetch('/api/data.json')
    r'(?:fetch|axios|xhr)\s*\(\s*["\']([^"\']+)["\']',
    
    # Route definitions: route: '/admin/dashboard'
    r'route\s*:\s*["\']([^"\']+)["\']',
    
    # URL constants: const API_URL = '/api/v2'
    r'(?:URL|PATH|ENDPOINT)\s*=\s*["\']([^"\']+)["\']',
]

LINKFINDER_REGEX = re.compile(LINKFINDER_REGEX_STR, re.VERBOSE)
SIMPLE_REGEX = re.compile(r"""(?:"|'|`)(/[a-zA-Z0-9_?&=\/\-\#\.]*)(?:"|'|`)""")
MODERN_REGEX_LIST = [re.compile(pattern, re.IGNORECASE) for pattern in MODERN_JS_PATTERNS]

# ... –ø–æ—Å–ª–µ MODERN_REGEX_LIST

# --- –ù–û–í–´–ô –≠–¢–ê–ü: –ü–æ–∏—Å–∫ API-–∫–ª—é—á–µ–π ---
API_KEY_PATTERNS = [
    # === –ü–†–ò–í–ê–¢–ù–´–ï –ö–õ–Æ–ß–ò (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∏–º–ø–∞–∫—Ç, 100% —Ç–æ—á–Ω–æ—Å—Ç—å) ===
    ('RSA Private Key', r'-----BEGIN (?:RSA )?PRIVATE KEY-----'),
    ('SSH Private Key', r'-----BEGIN OPENSSH PRIVATE KEY-----'),
    ('PGP Private Key', r'-----BEGIN PGP PRIVATE KEY BLOCK-----'),
    ('Firebase Admin SDK', r'"private_key":\s*"-----BEGIN PRIVATE KEY-----'),
    
    # === AWS (—Å—Ç—Ä–æ–≥–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã) ===
    ('AWS Access Key ID', r'AKIA[A-Z0-9]{16}'),
    ('AWS Secret Key', r'(?:aws_secret_access_key|AWS_SECRET_ACCESS_KEY)["\'\s]*[:=]["\'\s]*["\']([A-Za-z0-9/+=]{40})["\']'),
    
    # === GOOGLE CLOUD ===
    ('Google Cloud Service Account', r'"type":\s*"service_account"'),
    
    # === AZURE ===
    ('Azure Client Secret', r'(?:AZURE_CLIENT_SECRET|azure_client_secret)["\'\s]*[:=]["\'\s]*["\']([A-Za-z0-9\-\.~_]{32,})["\']'),
    
    # === GITHUB (–≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å) ===
    ('GitHub PAT', r'ghp_[a-zA-Z0-9]{36,}'),
    ('GitHub OAuth', r'gho_[a-zA-Z0-9]{36,}'),
    ('GitHub App Token', r'ghs_[a-zA-Z0-9]{36,}'),
    ('GitHub Refresh', r'ghr_[a-zA-Z0-9]{36,}'),
    
    # === GITLAB ===
    ('GitLab PAT', r'glpat-[a-zA-Z0-9\-_]{20,}'),
    ('GitLab Runner', r'glrt-[a-zA-Z0-9\-_]{20,}'),
    
    # === –ü–õ–ê–¢–ï–ñ–ò (100% —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã) ===
    ('Stripe Live Secret', r'sk_live_[a-zA-Z0-9]{24,}'),
    ('Stripe Restricted', r'rk_live_[a-zA-Z0-9]{24,}'),
    ('Square Access Token', r'sq0atp-[a-zA-Z0-9\-_]{22,}'),
    ('Square Refresh Token', r'sq0csp-[a-zA-Z0-9\-_]{43,}'),
    ('Braintree Token', r'access_token\$production\$[a-z0-9]{16}\$[a-f0-9]{32}'),
    
    # === –ú–ï–°–°–ï–ù–î–ñ–ï–†–´ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã) ===
    ('Slack Bot Token', r'xoxb-[0-9]{10,13}-[0-9]{10,13}-[a-zA-Z0-9]{24,}'),
    ('Slack Webhook', r'https://hooks\.slack\.com/services/T[A-Z0-9]{8,}/B[A-Z0-9]{8,}/[a-zA-Z0-9]{24,}'),
    ('Discord Bot Token', r'[MN][A-Za-z\d]{23,25}\.[A-Za-z\d]{6}\.[A-Za-z\d_-]{27,}'),
    ('Discord Webhook', r'https://discord(?:app)?\.com/api/webhooks/\d{17,19}/[A-Za-z0-9_-]{60,68}'),
    ('Telegram Bot', r'\d{8,10}:[A-Za-z0-9_-]{35}'),
    
    # === EMAIL (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã) ===
    ('SendGrid', r'SG\.[a-zA-Z0-9\-_]{22,}\.[a-zA-Z0-9\-_]{43,}'),
    ('Mailgun', r'key-[a-f0-9]{32}'),
    ('Mailchimp', r'[a-f0-9]{32}-us\d{1,2}'),
    
    # === –ë–ê–ó–´ –î–ê–ù–ù–´–• (connection strings) ===
    ('MongoDB', r'mongodb(?:\+srv)?://[a-zA-Z0-9\-_]+:[^@\s]+@[a-zA-Z0-9\-_.]+'),
    ('Redis', r'redis://:[^@\s]+@[a-zA-Z0-9\-_.]+:\d+'),
    ('PostgreSQL', r'postgres(?:ql)?://[a-zA-Z0-9\-_]+:[^@\s]+@[a-zA-Z0-9\-_.]+'),
    
    # === HOSTING ===
    ('DigitalOcean', r'dop_v1_[a-f0-9]{64}'),
    
    # === TWILIO (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã) ===
    ('Twilio SID', r'AC[a-f0-9]{32}'),
    ('Twilio Auth', r'SK[a-f0-9]{32}'),
    
    # === GENERIC (—Ç–æ–ª—å–∫–æ —Å –Ø–í–ù–´–ú–ò –∏–º–µ–Ω–∞–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö) ===
    ('OAuth Client Secret', r'(?:CLIENT_SECRET|client_secret)["\'\s]*[:=]["\'\s]*["\']([A-Za-z0-9\-_.~]{32,})["\']'),
    ('App Secret', r'(?:APP_SECRET|app_secret)["\'\s]*[:=]["\'\s]*["\']([A-Za-z0-9\-_.~]{32,})["\']'),
]
# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
COMPILED_API_KEY_PATTERNS = [(key_type, re.compile(pattern)) for key_type, pattern in API_KEY_PATTERNS]

def extract_api_keys(content: str):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç API –∫–ª—é—á–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    found_keys = []
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, —Ç–∞–∫ –∫–∞–∫ beautifier –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–ª—é—á–µ–π
    for key_type, pattern in COMPILED_API_KEY_PATTERNS:
        matches = pattern.finditer(content)
        for match in matches:
            # –ï—Å–ª–∏ –≤ regex –µ—Å—Ç—å –≥—Ä—É–ø–ø–∞, –±–µ—Ä–µ–º –µ–µ, –∏–Ω–∞—á–µ - –≤—Å—é —Å—Ç—Ä–æ–∫—É
            key_value = match.group(1) if match.groups() else match.group(0)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
            if not any(placeholder in key_value.lower() for placeholder in 
                      ['xxx', 'your_', 'example', 'demo', 'test', '<', '>']):
                found_keys.append((key_type, key_value))
    return found_keys

# --- –ù–û–í–´–ô –≠–¢–ê–ü: –ê–Ω–∞–ª–∏–∑ Sourcemaps –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ ---

async def check_and_extract_sourcemap(session, js_url, content):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç source map, –≤–æ–∑–≤—Ä–∞—â–∞—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥"""
    sourcemap_match = re.search(r'//# sourceMappingURL=(.+)', content)
    if not sourcemap_match:
        return None
    
    sourcemap_ref = sourcemap_match.group(1).strip()
    
    sourcemap_content = None
    if sourcemap_ref.startswith('data:'):
        try:
            import base64
            encoded = sourcemap_ref.split(',')[1]
            decoded = base64.b64decode(encoded).decode('utf-8', 'ignore')
            sourcemap_content = json.loads(decoded)
        except Exception:
            return None
    else:
        sourcemap_url = urljoin(js_url, sourcemap_ref)
        try:
            async with session.get(sourcemap_url, timeout=30, ssl=False) as resp:
                if resp.status == 200:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º content_type=None, —á—Ç–æ–±—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    sourcemap_content = await resp.json(content_type=None)
        except Exception:
            return None

    if not sourcemap_content or 'sourcesContent' not in sourcemap_content:
        return None
    
    sources = []
    if 'sourcesContent' in sourcemap_content:
        for i, source_code in enumerate(sourcemap_content['sourcesContent']):
            if source_code:
                source_name = sourcemap_content['sources'][i] if i < len(sourcemap_content['sources']) else f"source_{i}"
                sources.append({'name': source_name, 'content': source_code})

    return {'sources': sources} if sources else None


# --- –≠–¢–ê–ü 1: –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π (–∏–∑ v2) ---
FP_EXACT_MATCHES = {
    'application/json', 'application/xml', 'application/octet-stream', 'application/pdf',
    'application/x-www-form-urlencoded', 'multipart/form-data', 'text/html', 'text/plain',
    'text/xml', 'text/css', 'text/javascript', 'image/png', 'image/jpeg', 'image/gif',
    'image/webp', 'image/svg', 'image/x-icon', 'font/woff2', 'text/partytown', 'text/x-component',
    'http://www.w3.org/2000/svg', 'http://www.w3.org/1999/xhtml', 'http://www.w3.org/1999/xlink',
    'http://www.w3.org/XML/1998/namespace', 'http://www.w3.org/2000/xmlns/', 'http://www.w3.org/1998/Math/MathML',
    'http://schema.org/', 'https://schema.org',
}

FP_SUBSTRINGS = ['node_modules', '.scss', '.ts', '.tsx', '.jsx', '.vue', 'source/src', '.js', '/blog/', '/ads/']
FP_ENDS_WITH = ['/index', '/core', '/utils', '/vendor.js', '/runtime.js', '/polyfills.js', '/styles.css']
FP_REGEX_PATTERNS = [
    re.compile(r'^(?:America|Europe|Asia|Africa|Australia|Atlantic|Pacific|Indian|Etc)/[A-Za-z_]+(?:\|[A-Za-z_/]+)*$'),
    re.compile(r'^(?:[A-Z]{2,4}[/]){2,}[A-Z]{2,4}$'), re.compile(r'^\/\d{3}$'),
    re.compile(r'(?:/i18n|/locale|/locales)'), re.compile(r'^[./]*[a-z]{2}(?:-[A-Z]{2})?(?:\.js(?:on)?)?$'),
    re.compile(r'-\w{8,}\.(?:css|css\.map)$'), re.compile(r'chunk-[A-Z0-9]{8,}\.js$'),
    re.compile(r'^[A-Za-z0-9+=]{20,}$'),
    re.compile(r'\.(svg|jpg|jpeg|png|webp|gif|ico|woff|woff2|ttf|eot|css|map|mp4|mp3|wav|d\.ts|html|htm|xml)$', re.IGNORECASE),
    re.compile(r'^[^a-zA-Z]+$'), re.compile(r'/[a-zA-Z]{2}-[a-zA-Z]{2}/'),
]

def filter_false_positives(endpoints: list, args: argparse.Namespace) -> list:
    if args.debug: print(f"[DEBUG][FP Filter] Starting FP filtering for {len(endpoints)} endpoints...")
    filtered = []
    for ep in endpoints:
        ep_lower = ep.lower()
        if ep.startswith(('http:', 'https:', '//')) or ep in {'/', './', '../'} or ep_lower in FP_EXACT_MATCHES: continue
        if any(sub in ep_lower for sub in FP_SUBSTRINGS) or any(ep_lower.endswith(suffix) for suffix in FP_ENDS_WITH): continue
        if any(p.search(ep) for p in FP_REGEX_PATTERNS): continue
        if '/' in ep and sum(1 for c in ep if c.isupper()) / len(ep.replace('/', '')) > 0.3: continue
        if len(ep) < 3 and ep.isalpha(): continue
        filtered.append(ep)
    if args.debug: print(f"[DEBUG][FP Filter] Finished FP filtering. Kept {len(filtered)} of {len(endpoints)} endpoints.")
    return filtered

# --- –≠–¢–ê–ü 2: Whitelist-—Ñ–∏–ª—å—Ç—Ä –¥–ª—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ (–∏–∑ v3) ---
API_WHITELIST_PATTERNS = [
    # === API –ø–∞—Ç—Ç–µ—Ä–Ω—ã ===
    r'.*\bapi\b.*', r'.*\brest\b.*', r'.*\bgraphql\b.*', r'.*/v\d{1,2}(/.*)?$', r'.*\bservice\b.*',
    r'.*\bservices\b.*', r'.*\bmicroservice\b.*', r'.*\bws\b.*', r'.*\bwebservice\b.*', r'.*\brpc\b.*',
    r'.*\bjsonrpc\b.*', r'.*\bxmlrpc\b.*', r'.*\bsoap\b.*', r'.*\bodata\b.*',
    # === –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ ===
    r'.*next.*api.*', r'.*nuxt.*api.*', r'.*\bstrapi\b.*', r'.*\bdirectus\b.*', r'.*\bghost\b.*',
    r'.*wp-json.*', r'.*\bwp\b.*',
    # === Admin –∏ –ø–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ===
    r'.*\badmin\b.*', r'.*\badministrator\b.*', r'.*\bmanagement\b.*', r'.*\bmanager\b.*',
    r'.*\bdashboard\b.*', r'.*\bpanel\b.*', r'.*\bcontrol\b.*', r'.*\bbackend\b.*', r'.*\bbackoffice\b.*',
    r'.*\bcp\b.*', r'.*\bconsole\b.*',
    # === –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ ===
    r'.*\binternal\b.*', r'.*\bprivate\b.*', r'.*\bsystem\b.*', r'.*\bsys\b.*', r'.*\bcore\b.*',
    r'.*\bconfig\b.*', r'.*\bconfiguration\b.*', r'.*\bsettings\b.*', r'.*\bpreferences\b.*', r'.*\boptions\b.*',
    # === –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ===
    r'.*\bauth\b.*', r'.*\bauthentication\b.*', r'.*\bauthorization\b.*', r'.*\blogin\b.*', r'.*\blogout\b.*',
    r'.*\bsignin\b.*', r'.*\bsignout\b.*', r'.*\bsignup\b.*', r'.*\bregister\b.*', r'.*\boauth\b.*',
    r'.*\boauth2\b.*', r'.*\bsaml\b.*', r'.*\bsso\b.*', r'.*\bopenid\b.*', r'.*\bjwt\b.*', r'.*\btoken\b.*',
    r'.*\brefresh\b.*',
    # === –§–∞–π–ª—ã –∏ –∑–∞–≥—Ä—É–∑–∫–∏ ===
    r'.*\bupload\b.*', r'.*\buploads\b.*', r'.*\bdownload\b.*', r'.*\bdownloads\b.*', r'.*\bfile\b.*',
    r'.*\bfiles\b.*', r'.*\bresources\b.*', r'.*\bcontent\b.*', r'.*\battachments\b.*', r'.*\bdocuments\b.*',
    # === Debug –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ===
    r'.*\bdebug\b.*', r'.*\btrace\b.*', r'.*\bhealth\b.*', r'.*\bstatus\b.*', r'.*\bping\b.*',
    r'.*\bmetrics\b.*', r'.*\bstats\b.*', r'.*\bstatistics\b.*', r'.*\bmonitor\b.*', r'.*\bmonitoring\b.*',
    r'.*\bactuator\b.*', r'.*\binfo\b.*',
    # === –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ===
    r'.*\bdb\b.*', r'.*\bdatabase\b.*', r'.*\bsql\b.*', r'.*\bquery\b.*', r'.*\bsearch\b.*',
    r'.*\belastic\b.*', r'.*\bes\b.*', r'.*\bmongo\b.*', r'.*\bredis\b.*', r'.*\binflux\b.*',
    # === –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –≤–µ–±-—Ö—É–∫–∏ ===
    r'.*\bwebhook\b.*', r'.*\bwebhooks\b.*', r'.*\bcallback\b.*', r'.*\bcallbacks\b.*', r'.*\bintegration\b.*',
    r'.*\bintegrations\b.*', r'.*\bconnect\b.*', r'.*\bsync\b.*', r'.*\bnotify\b.*', r'.*\bnotification\b.*',
    r'.*\bnotifications\b.*',
    # === –ú–æ–±–∏–ª—å–Ω—ã–µ API ===
    r'.*\bmobile\b.*', r'.*\bapp\b.*', r'.*\bandroid\b.*', r'.*\bios\b.*', r'.*\bdevice\b.*', r'.*\bdevices\b.*',
    # === –¢–µ—Å—Ç–æ–≤—ã–µ –∏ dev –æ–∫—Ä—É–∂–µ–Ω–∏—è ===
    r'.*\btest\b.*', r'.*\btesting\b.*', r'.*\bdev\b.*', r'.*\bdevelop\b.*', r'.*\bdevelopment\b.*',
    r'.*\bstage\b.*', r'.*\bstaging\b.*', r'.*\bsandbox\b.*', r'.*\bdemo\b.*', r'.*\bprototype\b.*',
    r'.*\bbeta\b.*', r'.*\balpha\b.*', r'.*\bpreview\b.*',
    # === –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã ===
    r'.*\.json(\?.*)?$', r'.*\.xml(\?.*)?$', r'.*\.rss(\?.*)?$', r'.*\.atom(\?.*)?$', r'.*\.txt(\?.*)?$',
    # === –ü–ª–∞—Ç–µ–∂–∏ –∏ e-commerce ===
    r'.*\bpayment\b.*', r'.*\bpayments\b.*', r'.*\bbilling\b.*', r'.*\binvoice\b.*', r'.*\binvoices\b.*',
    r'.*\border\b.*', r'.*\borders\b.*', r'.*\bcart\b.*', r'.*\bcheckout\b.*', r'.*\bsubscription\b.*',
    r'.*\bsubscriptions\b.*',
    # === –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏ –ø—Ä–æ—Ñ–∏–ª–∏ ===
    r'.*\buser\b.*', r'.*\busers\b.*', r'.*\bprofile\b.*', r'.*\bprofiles\b.*', r'.*\baccount\b.*',
    r'.*\baccounts\b.*', r'.*\bmember\b.*', r'.*\bmembers\b.*', r'.*\bcustomer\b.*', r'.*\bcustomers\b.*',
    # === –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç—ã ===
    r'.*\banalytics\b.*', r'.*\breport\b.*', r'.*\breports\b.*', r'.*\bexport\b.*', r'.*\bimport\b.*',
    r'.*\bbackup\b.*', r'.*\brestore\b.*',
    # === –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å ===
    r'.*\bsecurity\b.*', r'.*\bcsrf\b.*', r'.*\bxss\b.*', r'.*rate-limit.*', r'.*\bthrottle\b.*',
    # === –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã ===
    r'.*\bproxy\b.*', r'.*\btunnel\b.*', r'.*\bbridge\b.*', r'.*\bgateway\b.*', r'.*\bendpoint\b.*',
    r'.*\broute\b.*', r'.*\brouter\b.*',
    # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã ===
    r'.*\bemail\b.*', r'.*\bmail\b.*', r'.*\bpassword\b.*', r'.*\bpwd\b.*', r'.*\bsession\b.*',
    r'.*\bsessions\b.*', r'.*\bcookie\b.*', r'.*\bcookies\b.*', r'.*\bvalidate\b.*', r'.*\bvalidation\b.*',
    r'.*\berror\b.*', r'.*\berrors\b.*', r'.*\blogger\b.*', r'.*\blogging\b.*', r'.*\blogs\b.*',
]

DYNAMIC_PATTERNS = [
    # üî• –®–ê–ì 1: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ —Ç–∏–ø—ã –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤ –≤ {dynamic}
    # –≠—Ç–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç: {id}, {user-id}, {param_name}, {application-id} –∏ —Ç.–¥.
    (re.compile(r'\{[a-zA-Z0-9_-]+\}'), '{dynamic}'),
    
    # üî• –®–ê–ì 2: Template literals
    (re.compile(r'\$\{[^}]+\}'), '{dynamic}'),
    
    # üî• –®–ê–ì 3: Express/Router style –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (:id, :userId)
    (re.compile(r':([a-zA-Z_][\w]*)(?=/|$|\?)'), '{dynamic}'),
    
    # üî• –®–ê–ì 4: Bracket notation ([id], [userId])
    (re.compile(r'\[([a-zA-Z_][\w]*)\]'), '{dynamic}'),
    
    # üî• –®–ê–ì 5: –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è –∫–∞–≤—ã—á–∫–∞)
    (re.compile(r"['\"]([^'\"]*)['\"] *\+ *[a-zA-Z_][\w]*(?:\s*\+\s*['\"]|$)"), r'\1{dynamic}'),
]

# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
COMPILED_WHITELIST_PATTERNS = [re.compile(pattern, re.IGNORECASE) for pattern in API_WHITELIST_PATTERNS]

# --- –ù–û–í–´–ô –≠–¢–ê–ü: –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–∞–Ω–∞—Ü–∏—è ---

# Regex –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —á–∞—Å—Ç–µ–π `${...}` –∏ —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ `...`
DYNAMIC_PART_REGEX = re.compile(r'\$\{[^}]+\}')
TEMPLATE_LITERAL_REGEX = re.compile(r'`[^`]*`')

# –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –º—É—Å–æ—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∫–æ—Ä–µ–π (^, $)
SANITIZER_REGEX_PATTERNS = [
    re.compile(r'^[^/]*\{[^}]*:[^}]*\}[^/]*$'),  # CSS property:value –±–ª–æ–∫–∏
    re.compile(r'.*!important.*', re.IGNORECASE),
    re.compile(r'^\s*/\*.*\*/\s*$'),             # –ü–æ–ª–Ω—ã–µ CSS –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    re.compile(r'^\s*//.*$'),                    # –ü–æ–ª–Ω—ã–µ JS –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    re.compile(r'^\s*<!--.*-->\s*$'),            # –ü–æ–ª–Ω—ã–µ HTML –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    re.compile(r'^\s*(?:\*\s*)?(?:license|copyright|mit license|apache|gpl|bsd).*', re.IGNORECASE),
    re.compile(r'.*elements are self-closing.*', re.IGNORECASE),
    re.compile(r'.*Refer to our API for more information.*', re.IGNORECASE),
    re.compile(r'.*@(?:webkit|moz|ms|o)-.*'),    # CSS vendor prefixes
    re.compile(r'.*@(?:keyframes|media|import|charset).*'),  # CSS at-rules
    re.compile(r'^.{150,}$'),                    # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    re.compile(r'^\.(?:v-|scoped-|css-).*'),     # CSS –∫–ª–∞—Å—Å—ã
    re.compile(r'^[A-Za-z0-9+/]{40,}={0,2}$'),  # Base64
    re.compile(r'^[A-Fa-f0-9]{32,}$'),          # Hex hashes
]

# CSS-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
CSS_CONTEXT_SUBSTRINGS = [
    'margin-left', 'margin-right', 'padding-', 'font-size', 'background-color',
    'border-', 'text-align', 'display:block', 'position:absolute', 'z-index:',
    'transform:', 'transition:', 'animation:', '@keyframes', '.scoped-vuetify'
]

def is_framework_template_or_junk(endpoint: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —à–∞–±–ª–æ–Ω–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –∏–ª–∏ CSS/JS –º—É—Å–æ—Ä–æ–º (V2 FIX)"""
    
    # === –ù–û–í–´–ï, –ë–û–õ–ï–ï –ù–ê–î–ï–ñ–ù–´–ï –ü–†–û–í–ï–†–ö–ò ===
    
    # 1. –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º JSDoc –∏ TypeScript –∫–æ–¥
    if any(p in endpoint for p in ['/**', '*/', 'export interface', 'export function', 'UrlParams', '@param', '@see', 'return {']):
        return True
        
    # 2. –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫–æ–¥ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–æ–≤)
    if any(p in endpoint for p in ['greedy:!', 'lookbehind:!', '(?:', r'\b', r'\s', r'\w']):
        # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–∏–µ, —á—Ç–æ–±—ã –Ω–µ –æ—Ç–±—Ä–æ—Å–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ URL —Å–æ —Å–ª–æ–≤–æ–º 'raw'
        if r'\w' in endpoint and 'raw' in endpoint.lower():
            pass # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π URL, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        else:
            return True

    # 3. –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ –º–Ω–æ–≥–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –¥–ª—è regex, —ç—Ç–æ –º—É—Å–æ—Ä
    if len(endpoint) > 30 and (endpoint.count('|') + endpoint.count('*') + endpoint.count('?') + endpoint.count('(')) > 5:
        return True
        
    # === –°–¢–ê–†–´–ï –ü–†–û–í–ï–†–ö–ò (–æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞) ===
    ep_lower = endpoint.lower()
    
    # Angular/Vue/React templates
    if any(p in ep_lower for p in ['{{', '}}', '=>', 'this.', '...map', '.component', '.service', 'ng-', 'v-', '(event)', '[property]']):
        return True
    
    # CSS/SCSS –∫–æ–¥
    if any(p in ep_lower for p in ['content:', 'display:', 'position:', 'width:', 'height:', 'margin:', 'padding:', 'border:', 'background:', '!important', 'px', 'rem', 'em']) and not endpoint.startswith('/'):
        return True
    
    # CSS-–ø–æ–¥–æ–±–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
    if re.match(r'^[&.*#:][\w\-{}>\s]+\{', endpoint.strip()):
        return True
        
    return False

def ultimate_pre_filter_and_sanitize(endpoints: list, args: argparse.Namespace) -> list:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–µ–π –∏ –ø—Ä–µ–¥-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    if args.debug:
        print(f"[DEBUG][Ultimate Sanitizer] Starting pre-filtering for {len(endpoints)} raw matches...")
    
    sanitized_endpoints = set()
    
    for ep in endpoints:
        original_ep = ep
        
        # üî• –®–ê–ì 0: –û—Ç—Å–µ–∏–≤–∞–µ–º –æ—á–µ–≤–∏–¥–Ω—ã–π –º—É—Å–æ—Ä –î–û —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–∏
        if is_framework_template_or_junk(ep):
            if args.debug:
                print(f"[DEBUG][Sanitizer] Rejected framework template/junk: {original_ep}")
            continue
        
        # 1. –£–¥–∞–ª—è–µ–º –æ–±–µ—Ä—Ç–∫–∏ `...` –∏ –æ—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        ep_sanitized = TEMPLATE_LITERAL_REGEX.sub('', ep).strip().strip('\'".,;()[]{}')
        if not ep_sanitized:
            continue
            
        # 2. –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–∏–Ω–∞–º–∏–∫–∏
        for pattern, replacement in DYNAMIC_PATTERNS:
            ep_sanitized = pattern.sub(replacement, ep_sanitized)
            
        # 3. üî• –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –ü–û–ß–ò–ù–ö–ê: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –æ—à–∏–±–∫–∏
        
        # –û—à–∏–±–∫–∞ ‚Ññ1: –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π placeholder –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä /api/users/{dynamic
        if ep_sanitized.endswith('{dynamic'):
            ep_sanitized += '}'
            
        # –û—à–∏–±–∫–∞ ‚Ññ2: –ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å–∫–æ–±–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä /api/{dynamic/details
        ep_sanitized = ep_sanitized.replace('{dynamic/', '{dynamic}/')
        
        # 4. üî• –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ –±–∞–ª–∞–Ω—Å —Å–∫–æ–±–æ–∫ –Ω–∞—Ä—É—à–µ–Ω - –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º
        if ep_sanitized.count('{') != ep_sanitized.count('}'):
            if args.debug:
                print(f"[DEBUG][Sanitizer] Rejected unbalanced braces: {original_ep} -> {ep_sanitized}")
            continue
        
        # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ regex —Ñ–∏–ª—å—Ç—Ä—ã
        if any(pattern.search(ep_sanitized) for pattern in SANITIZER_REGEX_PATTERNS):
            if args.debug: 
                print(f"[DEBUG][Ultimate Sanitizer] Rejected by regex: {original_ep}")
            continue
        
        # 6. CSS –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        ep_lower = ep_sanitized.lower()
        is_css_context = any(css_sub in ep_lower for css_sub in CSS_CONTEXT_SUBSTRINGS)
        if is_css_context and not ep_sanitized.startswith('/'):
            if args.debug: 
                print(f"[DEBUG][Ultimate Sanitizer] Rejected CSS context: {original_ep}")
            continue
        
        # 7. –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if not ep_sanitized.startswith('/'):
            if '/' in ep_sanitized and not ep_sanitized.startswith('http'):
                ep_sanitized = '/' + ep_sanitized.lstrip('/')
            else:
                if args.debug: 
                    print(f"[DEBUG][Ultimate Sanitizer] Invalid path format: {original_ep}")
                continue
        
        path_part = ep_sanitized.replace('/', '').replace('{dynamic}', '')
        if not path_part:
            continue
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤
        if len(path_part) > 3:
            upper_ratio = sum(1 for c in path_part if c.isupper()) / len(path_part)
            if upper_ratio > 0.5:
                if args.debug: 
                    print(f"[DEBUG][Ultimate Sanitizer] Rejected by upper case ratio {upper_ratio:.2f}: {original_ep}")
                continue
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–∏—Ñ—Ä
        if len(path_part) > 2:
            digit_ratio = sum(1 for c in path_part if c.isdigit()) / len(path_part)
            if digit_ratio > 0.8:
                if args.debug: 
                    print(f"[DEBUG][Ultimate Sanitizer] Rejected by digit ratio {digit_ratio:.2f}: {original_ep}")
                continue
        
        # 8. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        ep_cleaned = ep_sanitized.rstrip('/')
        if not ep_cleaned or ep_cleaned == '/' or len(ep_cleaned) < 2:
            continue
        
        sanitized_endpoints.add(ep_cleaned)
    
    final_list = sorted(list(sanitized_endpoints))
    if args.debug:
        print(f"[DEBUG][Ultimate Sanitizer] Finished pre-filtering. Kept {len(final_list)} of {len(endpoints)}.")
    return final_list

def filter_whitelist_endpoints(endpoints: list, args: argparse.Namespace) -> list:
    """
    –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ whitelist –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    """
    if args.debug:
        print(f"[DEBUG][Whitelist Filter] Starting whitelist filtering for {len(endpoints)} endpoints...")
    filtered = []
    for ep in endpoints:
        if not ep or len(ep) < 2 or not ep.startswith('/'): continue
        is_interesting = any(pattern.search(ep) for pattern in COMPILED_WHITELIST_PATTERNS)
        if is_interesting:
            filtered.append(ep)
            if args.debug: print(f"[DEBUG][Whitelist Filter] Match found: {ep}")
    if args.debug:
        print(f"[DEBUG][Whitelist Filter] Finished whitelist filtering. Kept {len(filtered)} of {len(endpoints)} endpoints.")
    return filtered

# --- CHECKPOINT FUNCTIONS ---
async def save_checkpoint(r: redis.Redis, cycle_id: str, batch_index: int, processed_files: list):
    """Save checkpoint data to Redis"""
    checkpoint_data = {
        'batch_index': batch_index,
        'processed_files': processed_files,
        'timestamp': time.time()
    }
    key = CHECKPOINT_KEY_TPL.format(cycle_id=cycle_id)
    await r.set(key, json.dumps(checkpoint_data), ex=3600)  # Expire after 1 hour

async def load_checkpoint(r: redis.Redis, cycle_id: str):
    """Load checkpoint data from Redis"""
    key = CHECKPOINT_KEY_TPL.format(cycle_id=cycle_id)
    data = await r.get(key)
    if data:
        return json.loads(data)
    return None

async def clear_checkpoint(r: redis.Redis, cycle_id: str):
    """Clear checkpoint data"""
    key = CHECKPOINT_KEY_TPL.format(cycle_id=cycle_id)
    await r.delete(key)

# --- CONTENT DEDUPLICATION ---
def calculate_content_hash(content: str) -> str:
    """Calculate MD5 hash of content for deduplication"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

async def is_content_processed(r: redis.Redis, content_hash: str) -> bool:
    """Check if content with this hash was already processed"""
    key = CONTENT_HASH_KEY_TPL.format(hash=content_hash)
    return await r.exists(key)

async def mark_content_processed(r: redis.Redis, content_hash: str):
    """Mark content hash as processed"""
    key = CONTENT_HASH_KEY_TPL.format(hash=content_hash)
    await r.set(key, "1", ex=86400)  # Expire after 24 hours


def find_all_js_sources(html_content: str, base_url: str, source_host: str):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞–∫ –≤–Ω–µ—à–Ω–∏–µ JS-—Ñ–∞–π–ª—ã, —Ç–∞–∫ –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ inline-—Å–∫—Ä–∏–ø—Ç—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –æ—á–µ—Ä–µ–¥—å –∞–Ω–∞–ª–∏–∑–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ inline-—Å–∫—Ä–∏–ø—Ç–æ–≤.
    """
    if not html_content:
        return []
    
    tasks_for_queue = []
    soup = BeautifulSoup(html_content, 'lxml')

    # 1. –í–Ω–µ—à–Ω–∏–µ JS —Ñ–∞–π–ª—ã
    for tag in soup.find_all(['script', 'link']):
        src = tag.get('src') or tag.get('href')
        if src and ('.js' in src or 'javascript' in src):
            full_url = urljoin(base_url, src)
            # --- –ù–û–í–ê–Ø –°–¢–†–û–ö–ê: –ü–†–û–í–ï–†–ö–ê –ü–û –ë–õ–û–ö-–õ–ò–°–¢–£ ---
            if any(blocked in urlparse(full_url).netloc for blocked in SCRIPT_BLOCKLIST_DOMAINS):
                continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç
            # –§–æ—Ä–º–∞—Ç –∑–∞–¥–∞—á–∏: (URL, —Ö–æ—Å—Ç, —Ä–µ—Ñ–µ—Ä–µ—Ä, –∫–æ–Ω—Ç–µ–Ω—Ç) - –∫–æ–Ω—Ç–µ–Ω—Ç None, –µ–≥–æ –Ω–∞–¥–æ —Å–∫–∞—á–∞—Ç—å
            tasks_for_queue.append((full_url, source_host, base_url, None))

    # 2. Inline-—Å–∫—Ä–∏–ø—Ç—ã
    for script in soup.find_all('script'):
        if not script.get('src'):  # –≠—Ç–æ inline script
            script_content = script.string
            # –ò—â–µ–º —Ç–æ–ª—å–∫–æ –≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏—Ö –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö
            if script_content and len(script_content) > 100 and any(
                keyword in script_content for keyword in 
                ['window.config', 'window.ENV', 'apiUrl', 'apiKey', 'endpoint', 'accessToken', 'graphql']
            ):
                # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–°–ü–û–õ–¨–ó–£–ï–ú –•–ï–® –î–õ–Ø URL ---
                script_hash = hashlib.md5(script_content.encode('utf-8')).hexdigest()
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π URL –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                virtual_url = f"{base_url}#inline-script-md5-{script_hash[:16]}"
                # --- –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---
                
                # –§–æ—Ä–º–∞—Ç –∑–∞–¥–∞—á–∏: (URL, —Ö–æ—Å—Ç, —Ä–µ—Ñ–µ—Ä–µ—Ä, –∫–æ–Ω—Ç–µ–Ω—Ç) - –∫–æ–Ω—Ç–µ–Ω—Ç —É–∂–µ –µ—Å—Ç—å!
                tasks_for_queue.append((virtual_url, source_host, base_url, script_content))
                
    return tasks_for_queue

async def crawl_for_js_links(session: aiohttp.ClientSession, base_url: str, semaphore: asyncio.Semaphore,
                            js_queue: asyncio.Queue, pbar_crawl: tqdm, analyzed_urls: set, args: argparse.Namespace,
                            timing: HumanLikeTiming, header_manager: SmartHeaders, hybrid_fetcher,
                            global_seen_js: set, lock: asyncio.Lock):
    try:
        source_host = urlparse(base_url).hostname
        if not source_host: 
            return

        final_html_content, final_url, initial_html_content = await balanced_stealth_fetch_content(
            session, base_url, semaphore, args, timing, header_manager, hybrid_fetcher, referer=base_url
        )

        if not final_html_content and not initial_html_content: return

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ URL-–æ–≤
        found_tasks = set()

        # –ü–∞—Ä—Å–∏–º –æ–±–∞ HTML, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if initial_html_content:
            tasks = find_all_js_sources(initial_html_content, base_url, source_host)
            for task in tasks: found_tasks.add(task)
            
        if final_html_content:
            tasks = find_all_js_sources(final_html_content, final_url, source_host)
            for task in tasks: found_tasks.add(task)

        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å
        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å, –∏—Å–ø–æ–ª—å–∑—É—è –≥–ª–æ–±–∞–ª—å–Ω—É—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            for task in found_tasks:
                url_to_check = task[0]  # URL –∑–∞–¥–∞—á–∏
                if url_to_check not in analyzed_urls:
                    async with lock:
                        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –¥—Ä—É–≥–æ–π –ø–æ—Ç–æ–∫ –¥–æ–±–∞–≤–∏–ª URL
                        if url_to_check not in global_seen_js:
                            global_seen_js.add(url_to_check)
                            await js_queue.put(task)
                    analyzed_urls.add(url_to_check)
                
    except Exception as e: 
        print(f"\n[!] Crawler error for {base_url}: {e}", file=sys.stderr)
    finally: 
        pbar_crawl.update(1)
        
# –ü–æ–ª–Ω–∞—è, –≥–æ—Ç–æ–≤–∞—è –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏
async def fetch_js_only(
    session: aiohttp.ClientSession, 
    url: str, 
    semaphore: asyncio.Semaphore, 
    args: argparse.Namespace,
    r: redis.Redis,  # <-- –î–æ–±–∞–≤–ª—è–µ–º Redis –∫–ª–∏–µ–Ω—Ç
    referer: str = None
):
    """
    –£–º–Ω—ã–π –∏ –±—ã—Å—Ç—Ä—ã–π —Ñ–µ—Ç—á–µ—Ä –¥–ª—è JS-—Ñ–∞–π–ª–æ–≤ —Å —ç–∫–æ–Ω–æ–º–∏–µ–π —Ç—Ä–∞—Ñ–∏–∫–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ETag/Last-Modified –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –±–µ–∑ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (content, is_new_content).
    """
    async with semaphore:
        headers = {
            'User-Agent': random.choice(REALISTIC_USER_AGENTS),
            'Accept': 'application/javascript, */*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        if referer:
            headers['Referer'] = referer

        # --- –ù–ê–ß–ê–õ–û –õ–û–ì–ò–ö–ò –≠–ö–û–ù–û–ú–ò–ò –¢–†–ê–§–ò–ö–ê ---
        url_hash = hashlib.md5(url.encode()).hexdigest()
        meta_key = f"js_meta:{url_hash}"

        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ ETag –∏ Last-Modified –∏–∑ Redis
            cached_meta = await r.hgetall(meta_key)
            if cached_meta:
                # Redis –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –ë–ê–ô–¢–´, –∏—Ö –Ω—É–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
                etag = cached_meta.get(b'etag')
                last_mod = cached_meta.get(b'last_modified')
                
                if etag:
                    headers['If-None-Match'] = etag.decode('utf-8')
                if last_mod:
                    headers['If-Modified-Since'] = last_mod.decode('utf-8')
        except redis.RedisError as e:  # <--- –¢–µ–ø–µ—Ä—å except –Ω–∞ —É—Ä–æ–≤–Ω–µ try (4 –ø—Ä–æ–±–µ–ª–∞ –ø–µ—Ä–µ–¥ –Ω–∏–º)
            if args.debug:
                print(f"[DEBUG][Redis] Error getting meta for {url}: {e}")
            # cached_meta –∑–¥–µ—Å—å –Ω–µ –Ω—É–∂–µ–Ω, –ø—Ä–æ—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É

        try:
            async with session.get(url, headers=headers, timeout=NORMAL_TIMEOUT, ssl=False, allow_redirects=True) as resp:
                
                # --- –û–ë–†–ê–ë–û–¢–ö–ê –û–¢–í–ï–¢–ê –°–ï–†–í–ï–†–ê ---
                if resp.status == 304:  # 304 Not Modified
                    if args.debug:
                        print(f"[DEBUG][HTTP 304] Content not changed for {url}. Skipping analysis.")
                    
                    # –í–∞–∂–Ω–æ! –ü—Ä–æ–¥–ª–µ–≤–∞–µ–º –∂–∏–∑–Ω—å —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ Redis, —á—Ç–æ–±—ã –æ–Ω –Ω–µ –∏—Å—á–µ–∑
                    canonical_url_for_hash = get_canonical_url(url)
                    url_hash_key_for_diff = f"js_diff_hash:{hashlib.md5(canonical_url_for_hash.encode('utf-8', 'ignore')).hexdigest()}"
                    
                    last_hash_bytes = await r.get(url_hash_key_for_diff)
                    if last_hash_bytes:
                        last_hash = last_hash_bytes.decode()
                        old_content_key = f"js_diff_body:{last_hash}"
                        # –ü—Ä–æ–¥–ª–µ–≤–∞–µ–º TTL –¥–ª—è —Ç–µ–ª–∞ —Ñ–∞–π–ª–∞ –∏ –¥–ª—è –∫–ª—é—á–∞ —Å —Ö–µ—à–µ–º
                        await r.expire(old_content_key, 86400 * 30)
                        await r.expire(url_hash_key_for_diff, 86400 * 30)

                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–ª–∞–≥, —á—Ç–æ–±—ã –≤–æ—Ä–∫–µ—Ä —Ç–æ—á–Ω–æ –∑–Ω–∞–ª, —á—Ç–æ —ç—Ç–æ 304
                    return None, False, "304_not_modified"

                if resp.status == 200:
                    content_type = resp.headers.get('Content-Type', '').lower()
                    if 'javascript' in content_type or 'application/x-javascript' in content_type:
                        content = await resp.text(encoding='utf-8', errors='ignore')
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ ETag –∏ Last-Modified –¥–ª—è –±—É–¥—É—â–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
                        new_etag = resp.headers.get('ETag')
                        new_last_modified = resp.headers.get('Last-Modified')
                        
                        pipe = r.pipeline()
                        if new_etag:
                            pipe.hset(meta_key, 'etag', new_etag)
                        if new_last_modified:
                            pipe.hset(meta_key, 'last_modified', new_last_modified)
                        
                        if pipe.command_stack:
                           pipe.expire(meta_key, 86400 * 30) # –•—Ä–∞–Ω–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ 30 –¥–Ω–µ–π
                           await pipe.execute()

                        return content, True, None
                
                return None, False, None

        except Exception as e:
                if args.debug:
                    # –¢–µ–ø–µ—Ä—å –º—ã –≤—ã–≤–æ–¥–∏–º –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–∏–ø –æ—à–∏–±–∫–∏, –Ω–æ –∏ –ø–æ–ª–Ω—ã–π traceback
                    print(f"\n[JS_FETCHER_FAIL] Failed to get {url}: {type(e).__name__}")
                    # –í—ã–≤–æ–¥–∏–º traceback –≤ stderr, —á—Ç–æ–±—ã –æ–Ω –Ω–µ –º–µ—à–∞–ª –æ—Å–Ω–æ–≤–Ω–æ–º—É –ª–æ–≥—É
                    traceback.print_exc(file=sys.stderr)
                return None, False, None

def parser_file(content: str, args: argparse.Namespace):
    if not content: return []
    if args.debug: print(f"[DEBUG][Parser] Processing content of length {len(content)}")
    
    # <<< –ù–û–í–´–ô Regex –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ –∏–∑ JSDoc –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ >>>
    JSDOC_ENDPOINT_REGEX = re.compile(r'^\s*\*\s+(?:GET|POST|PUT|DELETE|PATCH|HEAD)\s+([a-zA-Z0-9/\-_{}]+)', re.MULTILINE)

    try:
        beautified_content = jsbeautifier.beautify(content) if len(content) <= 1000000 else content.replace(";",";\r\n").replace(",",",\r\n")
    except Exception as e:
        if args.debug: print(f"[DEBUG][Parser] Beautification failed: {e}, using raw content")
        beautified_content = content
    
    matches = set()
    
    # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    matches.update(m.group(1).strip() for m in LINKFINDER_REGEX.finditer(beautified_content) if m.group(1))
    matches.update(m.group(1).strip() for m in SIMPLE_REGEX.finditer(beautified_content) if m.group(1))
    matches.update(m.group(1).strip() for m in LINKFINDER_REGEX.finditer(content) if m.group(1))
    matches.update(m.group(1).strip() for m in SIMPLE_REGEX.finditer(content) if m.group(1))
    
    for regex in MODERN_REGEX_LIST:
        for match in regex.finditer(beautified_content):
            endpoint = match.group(1).strip()
            if endpoint and endpoint.startswith('/'):
                matches.add(endpoint)
        for match in regex.finditer(content):
            endpoint = match.group(1).strip() 
            if endpoint and endpoint.startswith('/'):
                matches.add(endpoint)

    # <<< –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–π, —Ç–æ—á–Ω—ã–π JSDoc regex >>>
    for match in JSDOC_ENDPOINT_REGEX.finditer(content):
        endpoint = match.group(1).strip()
        if endpoint.startswith('/'):
            matches.add(endpoint)
    
    if args.debug: print(f"[DEBUG][Parser] Found {len(matches)} unique endpoints")
    return [ep for ep in matches if ep]




# === –£–ú–ù–´–ô RETRY –° BACKOFF ===
async def smart_fetch_with_retry(session, url: str, headers: dict, max_retries: int = 3):
    """Fetch —Å —É–º–Ω—ã–º retry –∏ exponential backoff"""
    for attempt in range(max_retries):
        try:
            timeout = 10 + (attempt * 5)
            async with session.get(url, headers=headers, timeout=timeout, ssl=False) as resp:
                if resp.status == 429:
                    delay = (2 ** attempt) * random.uniform(5, 10)
                    print(f"[WAF] Rate limit hit. Waiting {delay:.1f}s before retry {attempt+1}")
                    await asyncio.sleep(delay)
                    continue
                elif resp.status in {403, 406}:
                    if attempt < max_retries - 1:
                        delay = random.uniform(2, 5)
                        print(f"[WAF] Possible WAF block (status {resp.status}). Long delay {delay:.1f}s")
                        await asyncio.sleep(delay)
                        continue
                content = await resp.text(encoding='utf-8', errors='ignore')
                return content, str(resp.url), dict(resp.headers)
        except asyncio.TimeoutError:
            if attempt < max_retries - 1:
                delay = random.uniform(10, 30)
                await asyncio.sleep(delay)
                continue
        except Exception as e:
            if attempt < max_retries - 1:
                delay = random.uniform(5, 15)
                await asyncio.sleep(delay)
                continue
    return None, url, {}


def read_urls_from_file(filepath: str):
    if not os.path.exists(filepath): sys.exit(f"[!] Input file not found: {filepath}")
    with open(filepath, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip() and line.startswith('http')]

async def send_notify_alert_async(message: str, config_path: str = None):
    command = ['notify', '-bulk']
    if config_path: command.extend(['-pc', config_path])
    try:
        proc = await asyncio.create_subprocess_exec(*command, stdin=asyncio.subprocess.PIPE, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
        _, stderr = await proc.communicate(input=message.encode())
        if proc.returncode == 0: print("[+] Notification sent successfully via notify.")
        else: print(f"[!] Error sending notification via notify: {stderr.decode().strip()}", file=sys.stderr)
    except Exception as e: print(f"[!] An exception occurred while sending notification: {e}", file=sys.stderr)
    
# --- –ù–û–í–´–ô –≠–¢–ê–ü: –£–º–Ω—ã–π –ø—Ä–æ–±–∏–Ω–≥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ ---

# –ù–∞–±–æ—Ä—ã —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
DYNAMIC_TEST_VALUES = {
    'numeric_id': ['1', '123', '0', '999999'],
    'uuid': ['00000000-0000-0000-0000-000000000000', '123e4567-e89b-12d3-a456-426614174000'],
    'string_id': ['admin', 'user', 'test', 'me', 'current'],
    'hash': ['d41d8cd98f00b204e9800998ecf8427e', 'abc123'],
    'email': ['admin@example.com', 'test@test.com'],
    'filename': ['config.json', 'data.xml', 'backup.sql'],
    'version': ['v1', 'v2', 'latest', '1.0'],
}

def detect_parameter_type(endpoint_context):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ endpoint'–∞."""
    endpoint_lower = endpoint_context.lower()
    if any(word in endpoint_lower for word in ['user', 'account', 'profile', 'customer', 'member', 'id']): return 'numeric_id'
    if any(word in endpoint_lower for word in ['file', 'document', 'attachment', 'upload', 'download']): return 'filename'
    if any(word in endpoint_lower for word in ['version', 'api', 'v']): return 'version'
    if any(word in endpoint_lower for word in ['email', 'mail', 'notification']): return 'email'
    if any(word in endpoint_lower for word in ['uuid', 'guid', 'session', 'token']): return 'uuid'
    return 'numeric_id' # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —á–∏—Å–ª–æ–≤—ã–µ ID

def generate_probe_endpoints(sanitized_endpoint, max_variants=2):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç endpoints –¥–ª—è –ø—Ä–æ–±–∏–Ω–≥–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞
    if '{' in sanitized_endpoint or '}' in sanitized_endpoint:
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∫–æ–±–∫–∏, –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –≤ –≤–∏–¥–µ {dynamic}
        if sanitized_endpoint.count('{dynamic}') != sanitized_endpoint.count('{'):
            # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            print(f"[WARNING] Malformed dynamic placeholder: {sanitized_endpoint}")
            return [(sanitized_endpoint, f"{sanitized_endpoint} (malformed)")]
    
    if '{dynamic}' not in sanitized_endpoint:
        return [(sanitized_endpoint, sanitized_endpoint)]
    
    param_type = detect_parameter_type(sanitized_endpoint)
    test_values = DYNAMIC_TEST_VALUES[param_type][:max_variants]
    
    probe_endpoints = []
    for value in test_values:
        probe_url = sanitized_endpoint.replace('{dynamic}', str(value))
        display_name = f"{sanitized_endpoint} (probe: {value})"
        probe_endpoints.append((probe_url, display_name))
    
    return probe_endpoints

async def stealth_probe_endpoint(session, base_url, endpoint_info, semaphore, timing, header_manager):
    """–°—Ç–µ–ª—Å –≤–µ—Ä—Å–∏—è –ø—Ä–æ–±–∏–Ω–≥–∞ endpoints"""
    if isinstance(endpoint_info, tuple):
        probe_url, display_name = endpoint_info
    else:
        probe_url = display_name = endpoint_info
    full_url = urljoin(base_url, probe_url)
    async with semaphore:
        await timing.get_delay()
        headers = header_manager.get_headers(full_url)
        try:
            async with session.get(full_url, headers=headers, timeout=15, ssl=False, allow_redirects=True) as resp:
                status, content = resp.status, await resp.read()
                length, title = len(content), "N/A"
                header_manager.update_session(dict(resp.headers), full_url)
                if 'html' in resp.headers.get('Content-Type', '').lower():
                    try:
                        soup = BeautifulSoup(content.decode('utf-8', errors='ignore'), 'lxml')
                        title_tag = soup.find('title')
                        if title_tag and title_tag.string:
                            title = title_tag.string.strip().replace('\n', ' ').replace('\r', '')
                    except Exception:
                        title = "Parse Error"
                return display_name, status, length, title
        except Exception as e:
            return display_name, 0, 0, f"Request Error: {type(e).__name__}"

def upload_results(file_path):
    print(f"[+] Uploading {file_path} to gofile.io...")
    try:
        result = subprocess.run(["curl", "-s", "-F", f"file=@{file_path}", "https://store1.gofile.io/uploadFile"], capture_output=True, text=True, check=True, errors='ignore')
        data = json.loads(result.stdout)
        if data.get("status") == "ok":
            link = data.get("data", {}).get("downloadPage")
            print(f"[+] Upload successful: {link}")
            return link
    except Exception as e: print(f"[!] An exception occurred during file upload: {e}")
    return None


async def generate_report_and_notify(
    findings_dict: dict, 
    api_keys_dict: dict,
    args: argparse.Namespace, 
    session: aiohttp.ClientSession, 
    timing: HumanLikeTiming, 
    header_manager: SmartHeaders
):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç —Å —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞–º–∏ –∏ API-–∫–ª—é—á–∞–º–∏."""
    
    relevant_hosts = set(findings_dict.keys()) | set(api_keys_dict.keys())
    
    if not relevant_hosts:
        print("[+] No new endpoints or API keys found in this scan cycle.")
        return
    
    probe_semaphore = asyncio.Semaphore(args.threads * 2)
    total_new_endpoints = 0
    total_new_keys = 0
    total_probe_tasks = 0
    report_lines = []
    
    print("[+] Generating final report with all findings...")
    
    for host in sorted(list(relevant_hosts)):
        report_lines.append("----------------------------------------")
        report_lines.append(f"Host: {host}")

        # –°–µ–∫—Ü–∏—è API –∫–ª—é—á–µ–π
        host_keys = api_keys_dict.get(host, [])
        if host_keys:
            total_new_keys += len(host_keys)
            report_lines.append("\nüî• Found Potential API Keys:")
            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∫–ª—é—á–µ–π –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–æ–º
            for key_type, key_value, source in sorted(list(set(host_keys))):
                report_lines.append(f"  - Type: {key_type} | Key: {key_value} | In: {source}")
        
        # –°–µ–∫—Ü–∏—è —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
        endpoints = findings_dict.get(host, [])
        if not endpoints:
            continue
        
        report_lines.append("\nüì° Probing New Endpoints:")
        
        base_url = f"https://{host}"
        _, status, length, title = await stealth_probe_endpoint(session, base_url, "/", probe_semaphore, timing, header_manager)
        if "Request Error" in title:
            print(f"[!] Baseline probe for {host} failed: {title}. Skipping host.", file=sys.stderr)
            report_lines.append(f"  BASELINE: / - PROBE FAILED: {title}")
        else:
            report_lines.append(f"  BASELINE: / - {title} - {status} - {length}")
            
        unique_endpoints = sorted(list(set(endpoints)))
        total_new_endpoints += len(unique_endpoints)
        
        all_probe_tasks = []
        for ep in unique_endpoints:
            probe_variants = generate_probe_endpoints(ep, max_variants=2)
            for probe_info in probe_variants:
                task = asyncio.create_task(stealth_probe_endpoint(session, base_url, probe_info, probe_semaphore, timing, header_manager))
                all_probe_tasks.append(task)
        
        total_probe_tasks += len(all_probe_tasks)
        if not all_probe_tasks: continue

        results = await asyncio.gather(*all_probe_tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                display_name, status, length, title = "Unknown endpoint", 0, 0, f"Error: {type(result).__name__}"
            else:
                display_name, status, length, title = result
            
            report_lines.append(f"  {display_name} - {title} - {status} - {length}")

    if not report_lines: return
    
    report_lines.append("----------------------------------------")
    report_content = "\n".join(report_lines)
    
    header_parts = []
    if total_new_endpoints > 0:
        header_parts.append(f"{total_new_endpoints} new endpoints (probed {total_probe_tasks} variants)")
    if total_new_keys > 0:
        header_parts.append(f"{total_new_keys} potential API keys")

    if not header_parts: return

    header = f"JS-Analyzer found " + " and ".join(header_parts) + "."
    message = f"{header}\n\n```{report_content}```" if len(report_content) < 3500 else ""
    
    if not message:
        tmp_file_path, upload_link_success = None, False
        try:
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix="_js_report.txt", encoding='utf-8') as tmp_file:
                tmp_file.write(report_content)
                tmp_file_path = tmp_file.name
            upload_link = await asyncio.get_running_loop().run_in_executor(None, upload_results, tmp_file_path)
            if upload_link:
                message, upload_link_success = f"{header}\nFull report is too large. Download it here: {upload_link}", True
            else:
                message = f"{header}\nFull report is too large. Failed to upload. Report saved locally at: {tmp_file_path}"
        except Exception as e:
            message = f"{header}\nError handling large report file: {e}"
        finally:
            if upload_link_success and tmp_file_path and os.path.exists(tmp_file_path):
                os.remove(tmp_file_path)
                
    await send_notify_alert_async(message, args.notify_provider_config)

async def analyzer_worker_streaming(
    worker_id: int, session: aiohttp.ClientSession, r: redis.Redis,
    semaphore: asyncio.Semaphore, js_queue: asyncio.Queue,
    all_endpoints: dict, all_api_keys: dict,
    lock: asyncio.Lock, pbar_analyze: tqdm, args: argparse.Namespace,
    file_lock: asyncio.Lock, new_endpoints_file,
    timing: HumanLikeTiming, header_manager: SmartHeaders
):
    """
    –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤–æ—Ä–∫–µ—Ä–∞. –í—ã–ø–æ–ª–Ω—è–µ—Ç –î–í–ï –∑–∞–¥–∞—á–∏:
    1. –ê–Ω–∞–ª–∏–∑ DIFF: –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∫–æ–¥ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç diff –≤ AI.
    2. –ê–Ω–∞–ª–∏–∑ –ö–û–ù–¢–ï–ù–¢–ê: –ò—â–µ—Ç –Ω–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –∏ API-–∫–ª—é—á–∏ (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞).
    """
    
    async def analyze_source_code(code_content, source_name, host):
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª—é–±–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ JS-–∫–æ–¥–∞"""
        # 1. –ü–æ–∏—Å–∫ API –∫–ª—é—á–µ–π
        keys = extract_api_keys(code_content)
        if keys:
            async with lock:
                for key_type, key_value in keys:
                    all_api_keys[host].append((key_type, key_value, source_name))
        
        # 2. –ü–æ–∏—Å–∫ —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
        raw_eps = parser_file(code_content, args)
        sanitized_eps = ultimate_pre_filter_and_sanitize(raw_eps, args)
        pre_filtered_eps = filter_false_positives(sanitized_eps, args)
        endpoints = filter_whitelist_endpoints(pre_filtered_eps, args)
        
        if endpoints:
            redis_key = SEEN_ENDPOINTS_KEY_TPL.format(host=host)
            pipe = r.pipeline()
            for ep in endpoints: pipe.sadd(redis_key, ep)
            results = await pipe.execute()
            newly_added = [ep for i, ep in enumerate(endpoints) if results[i] == 1]
            
            if newly_added:
                async with lock:
                    all_endpoints[host].extend(newly_added)
                if args.debug and new_endpoints_file:
                    async with file_lock:
                        new_endpoints_file.write(f"\n--- New in {source_name} (from {host}) ---\n")
                        for ep in sorted(newly_added): new_endpoints_file.write(f"{ep}\n")

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≤–æ—Ä–∫–µ—Ä–∞
    while True:
        task_item = await js_queue.get()
        if task_item is None:
            js_queue.task_done()
            break
        
        lock_key = None # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏
        try:
            js_url, source_host, referer_url, inline_content = task_item
            pbar_analyze.set_description(f"Analyzing: {os.path.basename(urlparse(js_url).path)}")

            # === –ù–ê–ß–ê–õ–û –§–ò–ö–°–ê #1: –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ù–ê–Ø –ë–õ–û–ö–ò–†–û–í–ö–ê ===
            canonical_url_for_lock = get_canonical_url(js_url)
            lock_key_suffix = hashlib.md5(canonical_url_for_lock.encode('utf-8', 'ignore')).hexdigest()
            lock_key = f"lock:js_analyzer:{lock_key_suffix}"

            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –Ω–∞ 5 –º–∏–Ω—É—Ç (—Å –∑–∞–ø–∞—Å–æ–º –Ω–∞ –∞–Ω–∞–ª–∏–∑)
            lock_acquired = await r.set(lock_key, "1", nx=True, ex=300)
            if not lock_acquired:
                if args.debug:
                    print(f"[DEBUG] Lock for {js_url} is held by another worker, skipping.")
                continue # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å, –∑–Ω–∞—á–∏—Ç –¥—Ä—É–≥–æ–π –≤–æ—Ä–∫–µ—Ä —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥ —ç—Ç–∏–º —Ñ–∞–π–ª–æ–º
            # === –ö–û–ù–ï–¶ –§–ò–ö–°–ê #1 ===

            content = inline_content
            fetch_status = None # –î–ª—è inline-—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å—Ç–∞—Ç—É—Å –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω

            if not content:
                fetch_result = await fetch_js_only(
                                session, js_url, semaphore, args, r, referer=referer_url
                            )
                content, _, fetch_status = fetch_result if fetch_result else (None, False, None)
            
            if not content or fetch_status == "304_not_modified":
                continue

            # ==========================================================
            # === –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –° –°–ñ–ê–¢–ò–ï–ú (–°—Ç—Ä–∞—Ç–µ–≥–∏—è 1+3) ===
            # ==========================================================
            
            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            storage = HybridDiffStorage(r)
            
            # –í—ã–∑—ã–≤–∞–µ–º —É–º–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é: –æ–Ω–∞ —Å–∞–º–∞ —Ä–µ—à–∏—Ç, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª
            has_changes, diff, old_hash = await storage.get_and_compare(js_url, content)
            
            if not has_changes:
                # –§–∞–π–ª –ù–ï –∏–∑–º–µ–Ω–∏–ª—Å—è - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–µ—Å—å –∞–Ω–∞–ª–∏–∑
                if args.debug:
                    print(f"[DEBUG] Content unchanged for {js_url}, skipping all analysis.")
                continue
            
            # –§–∞–π–ª –ò–ó–ú–ï–ù–ò–õ–°–Ø - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∞–Ω–∞–ª–∏–∑
            print(f"\n[+] CHANGE DETECTED for: {js_url}")
            
            # 1. –ï—Å–ª–∏ –µ—Å—Ç—å diff –∏ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ AI-–∞–Ω–∞–ª–∏–∑
            if diff.strip() and old_hash:
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ diff (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
                if args.log_diffs:
                    try:
                        with open(args.log_diffs, "a", encoding="utf-8") as f:
                            f.write(f"\n--- DIFF FOR {js_url} ---\n")
                            f.write(diff)
                            f.write("\n--- END DIFF ---\n")
                    except Exception as log_e:
                        print(f"\n[!] Failed to write to diff log file: {log_e}", file=sys.stderr)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º diff –Ω–∞ AI-–∞–Ω–∞–ª–∏–∑
                ai_result = await send_diff_to_ai(session, diff, js_url, args)
                
                if ai_result and ai_result.get("signals_found"):
                    for signal in ai_result["signals_found"]:
                        llm_analysis = signal.get("llm_analysis", {})
                        # –í js_monitoring_simple_ai_v8.py

                        alert_message = llm_analysis.get("alert_message")

                        if alert_message:
                            feature_name = llm_analysis.get("feature_name", "N/A")
                            test_suggestion = llm_analysis.get("test_suggestion", "N/A")
                            location_inference = llm_analysis.get("location_inference", {})
                            technical_clues = llm_analysis.get("technical_clues", {})
                            
                            # üî• –ù–û–í–´–ô –§–û–†–ú–ê–¢ —Å confidence
                            full_alert = f"üî• {alert_message}\n\n"
                            full_alert += f"**Feature:** {feature_name}\n"
                            full_alert += f"**Change:** {llm_analysis.get('change_description', 'N/A')}\n"
                            
                            # üìç –ë–ª–æ–∫ "–ì–¥–µ –∏—Å–∫–∞—Ç—å" —Å CONFIDENCE
                            if location_inference:
                                confidence = location_inference.get('confidence', 'UNKNOWN')
                                
                                # –≠–º–æ–¥–∑–∏ –ø–æ —É—Ä–æ–≤–Ω—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                                confidence_emoji = {
                                    'HIGH': '‚úÖ',
                                    'MEDIUM': 'üî∂',
                                    'LOW': '‚ö†Ô∏è',
                                    'NONE': '‚ùì'
                                }.get(confidence, '‚ùì')
                                
                                full_alert += f"\nüìç **LOCATION INFERENCE** {confidence_emoji} Confidence: {confidence}\n"
                                
                                best_guess = location_inference.get('best_guess_url')
                                if best_guess:
                                    full_alert += f"  ‚Ä¢ URL: {best_guess}\n"
                                
                                likely_area = location_inference.get('likely_area')
                                if likely_area and likely_area != 'Unknown':
                                    full_alert += f"  ‚Ä¢ Area: {likely_area}\n"
                                
                                reasoning = location_inference.get('reasoning')
                                if reasoning:
                                    full_alert += f"  ‚Ä¢ Why: {reasoning}\n"
                            
                            # üîç –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É–ª–∏–∫–∏
                            if technical_clues:
                                endpoints = technical_clues.get('endpoints', [])
                                if endpoints:
                                    full_alert += f"\nüõ†Ô∏è **Technical Clues:**\n"
                                    full_alert += f"  ‚Ä¢ Endpoints: {', '.join(endpoints[:3])}\n"
                                
                                code_ids = technical_clues.get('code_identifiers', [])
                                if code_ids:
                                    full_alert += f"  ‚Ä¢ Code: {', '.join(code_ids[:3])}\n"
                            
                            full_alert += f"\nüéØ **Test:** {test_suggestion}\n"
                            full_alert += f"\nüîó **Source:** {llm_analysis.get('source_url', 'N/A')}"
                            
                            await send_notify_alert_async(full_alert, args.notify_provider_config)
            
            # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ endpoints –∏ API –∫–ª—é—á–∏
            await analyze_source_code(content, js_url, source_host)
            
            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ sourcemap (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ inline-—Å–∫—Ä–∏–ø—Ç)
            if not inline_content:
                sourcemap_data = await check_and_extract_sourcemap(session, js_url, content)
                
                if sourcemap_data:
                    if args.debug:
                        print(f"\n[DEBUG] Found sourcemap for {js_url} with {len(sourcemap_data['sources'])} files.")
                    
                    for source in sourcemap_data['sources']:
                        source_name_in_map = source.get('name', 'unknown_source')
                        source_content = source.get('content')
                        
                        if source_content:
                            await analyze_source_code(
                                source_content, 
                                f"{js_url} -> {source_name_in_map}", 
                                source_host
                            )
        
        except Exception as e:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º locals() —á—Ç–æ–±—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∏—Ç—å js_url, –¥–∞–∂–µ –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –±—ã–ª–∞ –¥–æ –µ–≥–æ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è
            url_for_error = locals().get('js_url', 'N/A')
            print(f"\n[!] Worker {worker_id} error processing {url_for_error}: {type(e).__name__}: {e}", file=sys.stderr)
        finally:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ –∑–∞—Ö–≤–∞—á–µ–Ω–∞
            if lock_key:
                await r.delete(lock_key)
            pbar_analyze.update(1)
            js_queue.task_done()

async def analyze_orchestrate(args):
    try:
        r = redis.Redis(host=args.redis_host, port=args.redis_port)
        await r.ping()
        print(f"[+] Connected to Redis for state tracking.")
    except Exception as e:
        sys.exit(f"[!] Redis connection failed: {e}")

    global_seen_js_urls = set()
    global_js_lock = asyncio.Lock()
    cycle_count = 0
    while True:
        cycle_count += 1
        timing = HumanLikeTiming()
        header_manager = SmartHeaders()
        start_time = time.monotonic()
        base_urls = read_urls_from_file(args.input)
        print(f"\n--- Starting scan cycle {cycle_count} with {len(base_urls)} base URLs at {time.strftime('%Y-%m-%d %H:%M:%S')} ---")

        new_endpoints_file = None
        if args.debug:
            new_endpoints_file = open("new_endpoints.txt", "w", encoding="utf-8")

        js_queue_filename = tempfile.mktemp(suffix="_js_queue.txt")
        found_js_count = 0

        try:
            # === –§–ê–ó–ê 1: –ö–†–ê–£–õ–ò–ù–ì ===
            print("\n--- Phase 1: Crawling for JS files ---")
            browser_handler = None
            hybrid_fetcher = None
            try:
                BATCH_SIZE = 100
                total_batches = math.ceil(len(base_urls) / BATCH_SIZE)
                
                crawl_semaphore = asyncio.Semaphore(args.threads * 2)
                connector = aiohttp.TCPConnector(limit_per_host=10, ssl=False, enable_cleanup_closed=True)
                
                async with aiohttp.ClientSession(connector=connector) as session:
                    analyzed_urls_in_cycle = set()
                    pbar_crawl = tqdm(total=len(base_urls), desc="Crawling URLs", unit="host", position=0)
                    
                    for batch_num in range(total_batches):
                        start_idx = batch_num * BATCH_SIZE
                        end_idx = min(start_idx + BATCH_SIZE, len(base_urls))
                        batch_urls = base_urls[start_idx:end_idx]
                        
                        print(f"\n[BATCH {batch_num + 1}/{total_batches}] Processing {len(batch_urls)} URLs...")
                        
                        browser_handler, hybrid_fetcher = await init_balanced_hybrid_system(
                            session, r, args.max_browser_concurrency
                        )
                        
                        js_temp_queue = asyncio.Queue(maxsize=2000)
                        
                        async def queue_to_file_writer():
                            nonlocal found_js_count
                            with open(js_queue_filename, 'a', encoding='utf-8') as f_queue:
                                while True:
                                    item = await js_temp_queue.get()
                                    if item is None:
                                        js_temp_queue.task_done()
                                        break
                                    f_queue.write(json.dumps(item) + '\n')
                                    found_js_count += 1
                                    js_temp_queue.task_done()
                        
                        writer_task = asyncio.create_task(queue_to_file_writer())
                        
                        crawler_tasks = [
                                                crawl_for_js_links(session, url, crawl_semaphore, js_temp_queue, pbar_crawl,
                                                                analyzed_urls_in_cycle, args, timing, header_manager, hybrid_fetcher,
                                                                global_seen_js_urls, global_js_lock)
                                                for url in batch_urls
                                            ]
                        await asyncio.gather(*crawler_tasks)
                        
                        await js_temp_queue.join()
                        await js_temp_queue.put(None)
                        await writer_task
                        
                        await cleanup_balanced_system(browser_handler, hybrid_fetcher)
                        browser_handler = None
                        hybrid_fetcher = None
                        
                        if batch_num < total_batches - 1: await asyncio.sleep(2)
                    
                    pbar_crawl.close()

            finally:
                if hybrid_fetcher and browser_handler:
                    await cleanup_balanced_system(browser_handler, hybrid_fetcher)
                try:
                    import psutil
                    def kill_browsers():
                        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                            proc_cmdline = proc.info.get('cmdline', [])
                            if proc_cmdline and any(k in ' '.join(proc_cmdline).lower() for k in ['chromium', 'playwright']):
                                print(f"[CLEANUP] Force killing browser process PID {proc.info['pid']}")
                                proc.kill()
                    await asyncio.get_running_loop().run_in_executor(None, kill_browsers)
                except (ImportError, Exception): pass
                
                print(f"[+] Discovery phase complete. Found {found_js_count} JS sources (files + inline).")

            # === –§–ê–ó–ê 2: –ê–ù–ê–õ–ò–ó ===
            if found_js_count > 0:
                print(f"\n--- Phase 2: Analyzing {found_js_count} JS sources ---")
                all_new_endpoints_by_host = defaultdict(list)
                all_new_api_keys_by_host = defaultdict(list)
                lock = asyncio.Lock()
                file_lock = asyncio.Lock()
                
                analysis_semaphore = asyncio.Semaphore(args.threads * 4)
                async with aiohttp.ClientSession() as analysis_session:
                    pbar_analyze = tqdm(total=found_js_count, desc="Analyzing JS", unit="source", position=0)
                    analysis_queue = asyncio.Queue()

                    analyzer_tasks = [
                        asyncio.create_task(
                            analyzer_worker_streaming(
                                worker_id, analysis_session, r, analysis_semaphore, analysis_queue,
                                all_new_endpoints_by_host, all_new_api_keys_by_host,
                                lock, pbar_analyze, args, file_lock, new_endpoints_file,
                                timing, header_manager
                            )
                        ) for worker_id in range(args.threads)
                    ]

                    print(f"[+] Loading {found_js_count} JS sources into analysis queue...")
                    with open(js_queue_filename, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip(): await analysis_queue.put(json.loads(line))
                    
                    for _ in range(args.threads): await analysis_queue.put(None)
                    await asyncio.gather(*analyzer_tasks)
                    pbar_analyze.close()

                print("[+] Analysis phase complete.")
                print("\n--- Final Report Generation ---")
                async with aiohttp.ClientSession() as report_session:
                    await generate_report_and_notify(
                        all_new_endpoints_by_host, all_new_api_keys_by_host,
                        args, report_session, timing, header_manager
                    )
        
        finally:
            if os.path.exists(js_queue_filename): os.remove(js_queue_filename)
            if new_endpoints_file: new_endpoints_file.close()
            await r.aclose()

        end_time = time.monotonic()
        print(f"[+] Scan cycle {cycle_count} finished in {end_time - start_time:.2f} seconds.")
        # === –í–´–í–û–î –°–¢–ê–¢–ò–°–¢–ò–ö–ò –•–†–ê–ù–ò–õ–ò–©–ê ===
        try:
            storage = HybridDiffStorage(r)
            stats = await storage.get_storage_stats()
            
            print("\n" + "="*50)
            print("üìä REDIS STORAGE STATISTICS")
            print("="*50)
            print(f"Redis Memory:      {stats.get('redis_memory', 'N/A')}")
            print(f"Redis Peak:        {stats.get('redis_peak', 'N/A')}")
            print(f"Redis Keys:        {stats.get('redis_keys', 'N/A')}")
            print(f"Redis Hash Keys:   {stats.get('redis_hash_keys', 'N/A')}")
            print(f"Disk Files:        {stats.get('disk_files', 'N/A')}")
            print(f"Disk Size:         {stats.get('disk_size_mb', 'N/A')} MB")
            print(f"Fragmentation:     {stats.get('fragmentation', 'N/A')}")
            print("="*50 + "\n")
        except Exception as stats_error:
            print(f"[!] Failed to get storage stats: {stats_error}", file=sys.stderr)

        if not args.continuous:
            print("\n[+] Single run complete. Exiting.")
            break
        print(f"\n[CONTINUOUS MODE] Waiting for {args.delay} seconds before the next run...")
        await asyncio.sleep(args.delay)

def build_parser():
    p = argparse.ArgumentParser(description="Production-ready JS Analyzer with batch processing and content deduplication.")
    p.add_argument("-i", "--input", required=True, help="Input file with base URLs.")
    p.add_argument("--redis-host", default="localhost", help="Redis server host.")
    p.add_argument("--redis-port", type=int, default=6379, help="Redis server port.")
    p.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help="Number of JS files to process per batch for checkpoint processing.")
    p.add_argument("--threads", type=int, default=3, help="Number of concurrent analyzer workers per batch.")
    p.add_argument("--continuous", action="store_true", help="Run the script in a continuous loop.")
    p.add_argument("--max-browser-concurrency", type=int, default=1, help="Maximum simultaneous browser fetches (1-2 recommended for stability).")
    p.add_argument("--delay", type=int, default=1, help="Delay in seconds between scans in continuous mode.")
    p.add_argument("-pc", "--notify-provider-config", help="Path to the notify provider-config file (optional).")
    p.add_argument("--log-diffs", help="Path to a file to log all detected code diffs (optional).")
    p.add_argument("--debug", action="store_true", help="Enable verbose debug logging and file output.")
    return p

def main():
    args = build_parser().parse_args()
    try: import lxml
    except ImportError: print("[!] 'lxml' not found. For better performance, run: pip install lxml")
    try: asyncio.run(analyze_orchestrate(args))
    except KeyboardInterrupt: print("\n[!] Analysis interrupted by user.", file=sys.stderr)

if __name__ == "__main__":
    main()